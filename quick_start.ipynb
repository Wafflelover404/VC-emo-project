{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wafflelover404/VC-emo-project/blob/main/quick_start.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ9tjpNEwlsn"
      },
      "source": [
        "# ðŸŽ­ Emotion Recognition - Quick Start\n",
        "\n",
        "This notebook allows you to quickly set up and train the emotion recognition model on Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qhkoV7swlsr"
      },
      "source": [
        "## 1. Clone Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTo3CLk3wlsr",
        "outputId": "51c22674-088b-4751-cfaf-cd5d39d5acd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'VC-emo-project'...\n",
            "remote: Enumerating objects: 34192, done.\u001b[K\n",
            "remote: Total 34192 (delta 0), reused 0 (delta 0), pack-reused 34192 (from 2)\u001b[K\n",
            "Receiving objects: 100% (34192/34192), 378.37 MiB | 10.03 MiB/s, done.\n",
            "Resolving deltas: 100% (28/28), done.\n",
            "Updating files: 100% (35979/35979), done.\n",
            "/content/VC-emo-project/VC-emo-project\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/Wafflelover404/VC-emo-project.git\n",
        "%cd VC-emo-project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVqCU0DNwlss"
      },
      "source": [
        "## 2. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzcMy6BLwlss"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q torch torchvision scikit-learn matplotlib pandas pillow opencv-python streamlit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4h9CrTewlss"
      },
      "source": [
        "## 3. Download Dataset (FER2013)\n",
        "\n",
        "The project uses the FER2013 dataset. Download and prepare it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RMv7P35wlss",
        "outputId": "d0c91f4d-3014-43c5-f965-2cc1e72b6e25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset folder already exists or download failed\n"
          ]
        }
      ],
      "source": [
        "# Option A: Download FER2013 from Kaggle (requires auth)\n",
        "# !pip install kaggle\n",
        "# !kaggle datasets download -d msambare/fer2013\n",
        "# !unzip -q fer2013.zip -d data/\n",
        "\n",
        "# Option B: Download from alternative source\n",
        "!wget -q https://www.dropbox.com/s/1w0j7pfr05eulc9/fer2013.tar.gz -O fer2013.tar.gz 2>/dev/null || echo \"Using alternative source...\"\n",
        "!tar -xzf fer2013.tar.gz 2>/dev/null && mv fer2013 data/ || echo \"Dataset folder already exists or download failed\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBLs1sV9wlss"
      },
      "source": [
        "## 4. Prepare Dataset Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7l86qirlwlst",
        "outputId": "902ecf8b-99dd-4b1f-f46f-17c25ae70f00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš ï¸ Dataset not found. Please upload manually or use alternative method.\n"
          ]
        }
      ],
      "source": [
        "# Create train/test folders if needed\n",
        "import os\n",
        "os.makedirs('train', exist_ok=True)\n",
        "os.makedirs('test', exist_ok=True)\n",
        "\n",
        "# Check if data exists\n",
        "if os.path.exists('data/fer2013'):\n",
        "    print(\"Dataset found at data/fer2013\")\n",
        "    print(os.listdir('data/fer2013')[:10])\n",
        "else:\n",
        "    print(\"âš ï¸ Dataset not found. Please upload manually or use alternative method.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBf5y-EWwlst"
      },
      "source": [
        "## 5. Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiV_4FbUwlst",
        "outputId": "65fe6133-c9a7-46d4-fade-abc662abb42d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training config:\n",
            "  EPOCHS: 30\n",
            "  IMG_SIZE: 224\n",
            "  BATCH_SIZE: 32\n",
            "  LR: 0.0003\n",
            "  WEIGHT_DECAY: 0.01\n",
            "  UNFREEZE: layer4\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Training parameters (modify as needed)\n",
        "EPOCHS = 30\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "LR = 0.0003\n",
        "WEIGHT_DECAY = 0.01\n",
        "UNFREEZE = \"layer4\"\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['EPOCHS'] = str(EPOCHS)\n",
        "os.environ['IMG_SIZE'] = str(IMG_SIZE)\n",
        "os.environ['BATCH_SIZE'] = str(BATCH_SIZE)\n",
        "os.environ['LR'] = str(LR)\n",
        "os.environ['WEIGHT_DECAY'] = str(WEIGHT_DECAY)\n",
        "os.environ['UNFREEZE'] = UNFREEZE\n",
        "\n",
        "print(f\"Training config:\")\n",
        "print(f\"  EPOCHS: {EPOCHS}\")\n",
        "print(f\"  IMG_SIZE: {IMG_SIZE}\")\n",
        "print(f\"  BATCH_SIZE: {BATCH_SIZE}\")\n",
        "print(f\"  LR: {LR}\")\n",
        "print(f\"  WEIGHT_DECAY: {WEIGHT_DECAY}\")\n",
        "print(f\"  UNFREEZE: {UNFREEZE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPa-Oz_rwlst"
      },
      "source": [
        "## 6. Run Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5r2_K_AIwlst",
        "outputId": "0cf1b4fc-d2d7-4f50-abdb-dc1e3318c7bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "[2026-02-14 10:47:42] Device: cuda:0\n",
            "[2026-02-14 10:47:42] FAST=0 IMG_SIZE=224 BATCH_SIZE=32 EPOCHS=30\n",
            "[2026-02-14 10:47:42] LR=0.0003 WEIGHT_DECAY=0.01 UNFREEZE=layer4\n",
            "[2026-02-14 10:47:42] Train samples: 28709  Test samples: 7178\n",
            "[2026-02-14 10:47:42] Train class distribution: angry=3995, disgust=436, fear=4097, happy=7215, neutral=4965, sad=4830, surprise=3171\n",
            "[2026-02-14 10:47:42] Epoch 1/30\n",
            "[2026-02-14 10:47:42] LR: 0.000150\n",
            "[2026-02-14 10:47:52]   train: batch 61/898  loss=1.9078  0.17s/batch  ETA=2.3m\n",
            "[2026-02-14 10:48:02]   train: batch 128/898  loss=1.9271  0.16s/batch  ETA=2.0m\n",
            "[2026-02-14 10:48:12]   train: batch 195/898  loss=1.5928  0.16s/batch  ETA=1.8m\n",
            "[2026-02-14 10:48:22]   train: batch 263/898  loss=1.4787  0.15s/batch  ETA=1.6m\n",
            "[2026-02-14 10:48:32]   train: batch 329/898  loss=1.8422  0.15s/batch  ETA=1.5m\n",
            "[2026-02-14 10:48:42]   train: batch 395/898  loss=1.8090  0.15s/batch  ETA=1.3m\n",
            "[2026-02-14 10:48:52]   train: batch 461/898  loss=1.5097  0.15s/batch  ETA=1.1m\n",
            "[2026-02-14 10:49:02]   train: batch 528/898  loss=1.6412  0.15s/batch  ETA=0.9m\n",
            "[2026-02-14 10:49:12]   train: batch 598/898  loss=1.8952  0.15s/batch  ETA=0.8m\n",
            "[2026-02-14 10:49:22]   train: batch 665/898  loss=1.8985  0.15s/batch  ETA=0.6m\n",
            "[2026-02-14 10:49:32]   train: batch 732/898  loss=1.7925  0.15s/batch  ETA=0.4m\n",
            "[2026-02-14 10:49:42]   train: batch 798/898  loss=1.7198  0.15s/batch  ETA=0.3m\n",
            "[2026-02-14 10:49:52]   train: batch 865/898  loss=1.5978  0.15s/batch  ETA=0.1m\n",
            "[2026-02-14 10:49:57] train Loss: 1.7573 Acc: 0.4592\n",
            "[2026-02-14 10:50:07]   test: batch 119/225  loss=1.7940  0.08s/batch  ETA=0.1m\n",
            "[2026-02-14 10:50:16] test Loss: 1.7455 Acc: 0.5288\n",
            "[2026-02-14 10:50:16] New best test acc: 0.5288 -> saved wafflelover404_emotion_model_best.pth\n",
            "[2026-02-14 10:50:16] Epoch time: 2.57 min\n",
            "[2026-02-14 10:50:16] Epoch 2/30\n",
            "[2026-02-14 10:50:16] LR: 0.000300\n",
            "[2026-02-14 10:50:26]   train: batch 70/898  loss=1.6963  0.14s/batch  ETA=2.0m\n",
            "[2026-02-14 10:50:36]   train: batch 138/898  loss=1.7331  0.15s/batch  ETA=1.8m\n",
            "[2026-02-14 10:50:46]   train: batch 204/898  loss=1.8108  0.15s/batch  ETA=1.7m\n",
            "[2026-02-14 10:50:56]   train: batch 271/898  loss=1.7572  0.15s/batch  ETA=1.5m\n",
            "[2026-02-14 10:51:06]   train: batch 337/898  loss=1.7886  0.15s/batch  ETA=1.4m\n",
            "[2026-02-14 10:51:16]   train: batch 404/898  loss=1.5800  0.15s/batch  ETA=1.2m\n",
            "[2026-02-14 10:51:26]   train: batch 473/898  loss=1.9431  0.15s/batch  ETA=1.1m\n",
            "[2026-02-14 10:51:36]   train: batch 540/898  loss=1.2547  0.15s/batch  ETA=0.9m\n",
            "[2026-02-14 10:51:46]   train: batch 606/898  loss=1.7736  0.15s/batch  ETA=0.7m\n",
            "[2026-02-14 10:51:56]   train: batch 673/898  loss=1.6913  0.15s/batch  ETA=0.6m\n",
            "[2026-02-14 10:52:07]   train: batch 740/898  loss=1.5908  0.15s/batch  ETA=0.4m\n",
            "[2026-02-14 10:52:17]   train: batch 807/898  loss=1.6935  0.15s/batch  ETA=0.2m\n",
            "[2026-02-14 10:52:27]   train: batch 877/898  loss=1.6884  0.15s/batch  ETA=0.1m\n",
            "[2026-02-14 10:52:30] train Loss: 1.6338 Acc: 0.5362\n",
            "[2026-02-14 10:52:40]   test: batch 122/225  loss=1.5822  0.08s/batch  ETA=0.1m\n",
            "[2026-02-14 10:52:49] test Loss: 1.6824 Acc: 0.5984\n",
            "[2026-02-14 10:52:49] New best test acc: 0.5984 -> saved wafflelover404_emotion_model_best.pth\n",
            "[2026-02-14 10:52:49] Epoch time: 2.55 min\n",
            "[2026-02-14 10:52:49] Epoch 3/30\n",
            "[2026-02-14 10:52:49] LR: 0.000300\n",
            "[2026-02-14 10:52:59]   train: batch 67/898  loss=1.4369  0.15s/batch  ETA=2.1m\n",
            "[2026-02-14 10:53:09]   train: batch 132/898  loss=1.6704  0.15s/batch  ETA=1.9m\n",
            "[2026-02-14 10:53:19]   train: batch 197/898  loss=1.4434  0.15s/batch  ETA=1.8m\n",
            "[2026-02-14 10:53:29]   train: batch 266/898  loss=1.3762  0.15s/batch  ETA=1.6m\n",
            "[2026-02-14 10:53:39]   train: batch 335/898  loss=1.6010  0.15s/batch  ETA=1.4m\n",
            "[2026-02-14 10:53:49]   train: batch 402/898  loss=1.7729  0.15s/batch  ETA=1.2m\n",
            "[2026-02-14 10:53:59]   train: batch 470/898  loss=1.6187  0.15s/batch  ETA=1.1m\n",
            "[2026-02-14 10:54:09]   train: batch 537/898  loss=1.6119  0.15s/batch  ETA=0.9m\n",
            "[2026-02-14 10:54:19]   train: batch 604/898  loss=1.6210  0.15s/batch  ETA=0.7m\n",
            "[2026-02-14 10:54:29]   train: batch 673/898  loss=1.0832  0.15s/batch  ETA=0.6m\n",
            "[2026-02-14 10:54:39]   train: batch 743/898  loss=1.7101  0.15s/batch  ETA=0.4m\n",
            "[2026-02-14 10:54:49]   train: batch 810/898  loss=1.2260  0.15s/batch  ETA=0.2m\n",
            "[2026-02-14 10:55:00]   train: batch 878/898  loss=1.7450  0.15s/batch  ETA=0.0m\n",
            "[2026-02-14 10:55:02] train Loss: 1.5605 Acc: 0.5875\n",
            "[2026-02-14 10:55:12]   test: batch 118/225  loss=1.8741  0.09s/batch  ETA=0.2m\n",
            "[2026-02-14 10:55:21] test Loss: 1.6542 Acc: 0.6130\n",
            "[2026-02-14 10:55:21] New best test acc: 0.6130 -> saved wafflelover404_emotion_model_best.pth\n",
            "[2026-02-14 10:55:21] Epoch time: 2.54 min\n",
            "[2026-02-14 10:55:21] Epoch 4/30\n",
            "[2026-02-14 10:55:21] LR: 0.000299\n",
            "[2026-02-14 10:55:31]   train: batch 68/898  loss=1.4615  0.15s/batch  ETA=2.0m\n",
            "[2026-02-14 10:55:41]   train: batch 138/898  loss=1.2082  0.15s/batch  ETA=1.8m\n",
            "[2026-02-14 10:55:51]   train: batch 206/898  loss=1.1664  0.15s/batch  ETA=1.7m\n",
            "[2026-02-14 10:56:02]   train: batch 273/898  loss=1.8352  0.15s/batch  ETA=1.5m\n",
            "[2026-02-14 10:56:12]   train: batch 340/898  loss=1.6145  0.15s/batch  ETA=1.4m\n",
            "[2026-02-14 10:56:22]   train: batch 408/898  loss=1.9033  0.15s/batch  ETA=1.2m\n",
            "[2026-02-14 10:56:32]   train: batch 476/898  loss=1.8214  0.15s/batch  ETA=1.0m\n",
            "[2026-02-14 10:56:42]   train: batch 546/898  loss=1.3008  0.15s/batch  ETA=0.9m\n",
            "[2026-02-14 10:56:52]   train: batch 615/898  loss=1.2667  0.15s/batch  ETA=0.7m\n",
            "[2026-02-14 10:57:02]   train: batch 682/898  loss=1.4526  0.15s/batch  ETA=0.5m\n",
            "[2026-02-14 10:57:12]   train: batch 749/898  loss=1.4431  0.15s/batch  ETA=0.4m\n",
            "[2026-02-14 10:57:22]   train: batch 816/898  loss=1.5609  0.15s/batch  ETA=0.2m\n",
            "[2026-02-14 10:57:32]   train: batch 883/898  loss=1.7501  0.15s/batch  ETA=0.0m\n",
            "[2026-02-14 10:57:34] train Loss: 1.4974 Acc: 0.6173\n",
            "[2026-02-14 10:57:44]   test: batch 120/225  loss=1.8039  0.08s/batch  ETA=0.1m\n",
            "[2026-02-14 10:57:53] test Loss: 1.6314 Acc: 0.6180\n",
            "[2026-02-14 10:57:53] New best test acc: 0.6180 -> saved wafflelover404_emotion_model_best.pth\n",
            "[2026-02-14 10:57:53] Epoch time: 2.52 min\n",
            "[2026-02-14 10:57:53] Epoch 5/30\n",
            "[2026-02-14 10:57:53] LR: 0.000296\n",
            "[2026-02-14 10:58:03]   train: batch 67/898  loss=1.1702  0.15s/batch  ETA=2.1m\n",
            "[2026-02-14 10:58:13]   train: batch 134/898  loss=1.5457  0.15s/batch  ETA=1.9m\n",
            "[2026-02-14 10:58:23]   train: batch 200/898  loss=0.9165  0.15s/batch  ETA=1.8m\n",
            "[2026-02-14 10:58:33]   train: batch 266/898  loss=1.1590  0.15s/batch  ETA=1.6m\n",
            "[2026-02-14 10:58:43]   train: batch 336/898  loss=1.4309  0.15s/batch  ETA=1.4m\n",
            "[2026-02-14 10:58:53]   train: batch 404/898  loss=1.4125  0.15s/batch  ETA=1.2m\n",
            "[2026-02-14 10:59:03]   train: batch 470/898  loss=1.5715  0.15s/batch  ETA=1.1m\n",
            "[2026-02-14 10:59:13]   train: batch 537/898  loss=1.6581  0.15s/batch  ETA=0.9m\n",
            "[2026-02-14 10:59:23]   train: batch 604/898  loss=1.8260  0.15s/batch  ETA=0.7m\n",
            "[2026-02-14 10:59:33]   train: batch 671/898  loss=1.5545  0.15s/batch  ETA=0.6m\n",
            "[2026-02-14 10:59:43]   train: batch 742/898  loss=1.3826  0.15s/batch  ETA=0.4m\n",
            "[2026-02-14 10:59:54]   train: batch 810/898  loss=1.0812  0.15s/batch  ETA=0.2m\n",
            "[2026-02-14 11:00:04]   train: batch 877/898  loss=1.8070  0.15s/batch  ETA=0.1m\n",
            "[2026-02-14 11:00:06] train Loss: 1.4493 Acc: 0.6457\n",
            "[2026-02-14 11:00:16]   test: batch 120/225  loss=1.9014  0.08s/batch  ETA=0.1m\n",
            "[2026-02-14 11:00:25] test Loss: 1.6439 Acc: 0.6266\n",
            "[2026-02-14 11:00:26] New best test acc: 0.6266 -> saved wafflelover404_emotion_model_best.pth\n",
            "[2026-02-14 11:00:26] Epoch time: 2.55 min\n",
            "[2026-02-14 11:00:26] Epoch 6/30\n",
            "[2026-02-14 11:00:26] LR: 0.000292\n",
            "[2026-02-14 11:00:36]   train: batch 67/898  loss=1.6562  0.15s/batch  ETA=2.1m\n",
            "[2026-02-14 11:00:46]   train: batch 134/898  loss=1.5390  0.15s/batch  ETA=1.9m\n",
            "[2026-02-14 11:00:56]   train: batch 205/898  loss=1.1827  0.15s/batch  ETA=1.7m\n",
            "[2026-02-14 11:01:06]   train: batch 271/898  loss=1.3761  0.15s/batch  ETA=1.6m\n",
            "[2026-02-14 11:01:16]   train: batch 336/898  loss=1.0435  0.15s/batch  ETA=1.4m\n",
            "[2026-02-14 11:01:26]   train: batch 403/898  loss=1.5316  0.15s/batch  ETA=1.2m\n",
            "[2026-02-14 11:01:36]   train: batch 470/898  loss=1.6600  0.15s/batch  ETA=1.1m\n",
            "[2026-02-14 11:01:46]   train: batch 538/898  loss=1.4350  0.15s/batch  ETA=0.9m\n",
            "[2026-02-14 11:01:56]   train: batch 607/898  loss=1.0533  0.15s/batch  ETA=0.7m\n",
            "[2026-02-14 11:02:06]   train: batch 674/898  loss=1.2564  0.15s/batch  ETA=0.6m\n",
            "[2026-02-14 11:02:16]   train: batch 740/898  loss=1.6505  0.15s/batch  ETA=0.4m\n",
            "[2026-02-14 11:02:26]   train: batch 807/898  loss=1.1100  0.15s/batch  ETA=0.2m\n",
            "[2026-02-14 11:02:36]   train: batch 874/898  loss=1.3122  0.15s/batch  ETA=0.1m\n",
            "[2026-02-14 11:02:40] train Loss: 1.3980 Acc: 0.6730\n",
            "[2026-02-14 11:02:50]   test: batch 120/225  loss=1.8644  0.08s/batch  ETA=0.1m\n",
            "[2026-02-14 11:02:58] test Loss: 1.6145 Acc: 0.6418\n",
            "[2026-02-14 11:02:59] New best test acc: 0.6418 -> saved wafflelover404_emotion_model_best.pth\n",
            "[2026-02-14 11:02:59] Epoch time: 2.55 min\n",
            "[2026-02-14 11:02:59] Epoch 7/30\n",
            "[2026-02-14 11:02:59] LR: 0.000285\n",
            "[2026-02-14 11:03:09]   train: batch 69/898  loss=1.5846  0.15s/batch  ETA=2.0m\n",
            "[2026-02-14 11:03:19]   train: batch 137/898  loss=0.9161  0.15s/batch  ETA=1.9m\n",
            "[2026-02-14 11:03:29]   train: batch 204/898  loss=1.3118  0.15s/batch  ETA=1.7m\n",
            "[2026-02-14 11:03:39]   train: batch 271/898  loss=1.5052  0.15s/batch  ETA=1.6m\n",
            "[2026-02-14 11:03:49]   train: batch 338/898  loss=1.5963  0.15s/batch  ETA=1.4m\n",
            "[2026-02-14 11:03:59]   train: batch 408/898  loss=1.4991  0.15s/batch  ETA=1.2m\n",
            "[2026-02-14 11:04:09]   train: batch 478/898  loss=1.2147  0.15s/batch  ETA=1.0m\n",
            "[2026-02-14 11:04:19]   train: batch 543/898  loss=1.5347  0.15s/batch  ETA=0.9m\n",
            "[2026-02-14 11:04:29]   train: batch 610/898  loss=1.4231  0.15s/batch  ETA=0.7m\n",
            "[2026-02-14 11:04:39]   train: batch 676/898  loss=1.3604  0.15s/batch  ETA=0.6m\n",
            "[2026-02-14 11:04:49]   train: batch 743/898  loss=1.0922  0.15s/batch  ETA=0.4m\n",
            "[2026-02-14 11:05:00]   train: batch 813/898  loss=1.6543  0.15s/batch  ETA=0.2m\n",
            "[2026-02-14 11:05:10]   train: batch 881/898  loss=1.6153  0.15s/batch  ETA=0.0m\n",
            "[2026-02-14 11:05:12] train Loss: 1.3505 Acc: 0.6964\n",
            "[2026-02-14 11:05:22]   test: batch 121/225  loss=1.5734  0.08s/batch  ETA=0.1m\n",
            "[2026-02-14 11:05:31] test Loss: 1.6275 Acc: 0.6453\n",
            "[2026-02-14 11:05:31] New best test acc: 0.6453 -> saved wafflelover404_emotion_model_best.pth\n",
            "[2026-02-14 11:05:31] Epoch time: 2.54 min\n",
            "[2026-02-14 11:05:31] Epoch 8/30\n",
            "[2026-02-14 11:05:31] LR: 0.000277\n",
            "[2026-02-14 11:05:41]   train: batch 66/898  loss=1.3450  0.15s/batch  ETA=2.1m\n",
            "[2026-02-14 11:05:51]   train: batch 132/898  loss=1.3340  0.15s/batch  ETA=1.9m\n",
            "[2026-02-14 11:06:01]   train: batch 201/898  loss=1.5518  0.15s/batch  ETA=1.7m\n",
            "[2026-02-14 11:06:11]   train: batch 271/898  loss=1.4515  0.15s/batch  ETA=1.6m\n",
            "[2026-02-14 11:06:21]   train: batch 337/898  loss=1.3671  0.15s/batch  ETA=1.4m\n",
            "[2026-02-14 11:06:31]   train: batch 403/898  loss=0.9709  0.15s/batch  ETA=1.2m\n",
            "[2026-02-14 11:06:41]   train: batch 470/898  loss=1.0722  0.15s/batch  ETA=1.1m\n",
            "[2026-02-14 11:06:52]   train: batch 538/898  loss=1.1822  0.15s/batch  ETA=0.9m\n",
            "[2026-02-14 11:07:02]   train: batch 607/898  loss=1.6108  0.15s/batch  ETA=0.7m\n",
            "[2026-02-14 11:07:12]   train: batch 676/898  loss=1.0405  0.15s/batch  ETA=0.6m\n",
            "[2026-02-14 11:07:22]   train: batch 742/898  loss=1.5237  0.15s/batch  ETA=0.4m\n",
            "[2026-02-14 11:07:32]   train: batch 808/898  loss=0.9951  0.15s/batch  ETA=0.2m\n",
            "[2026-02-14 11:07:42]   train: batch 874/898  loss=1.2880  0.15s/batch  ETA=0.1m\n",
            "[2026-02-14 11:07:45] train Loss: 1.3080 Acc: 0.7201\n",
            "[2026-02-14 11:07:55]   test: batch 120/225  loss=1.8408  0.08s/batch  ETA=0.1m\n",
            "[2026-02-14 11:08:04] test Loss: 1.6269 Acc: 0.6565\n",
            "[2026-02-14 11:08:04] New best test acc: 0.6565 -> saved wafflelover404_emotion_model_best.pth\n",
            "[2026-02-14 11:08:04] Epoch time: 2.55 min\n",
            "[2026-02-14 11:08:04] Epoch 9/30\n",
            "[2026-02-14 11:08:04] LR: 0.000267\n",
            "[2026-02-14 11:08:14]   train: batch 71/898  loss=1.1864  0.14s/batch  ETA=2.0m\n",
            "[2026-02-14 11:08:24]   train: batch 140/898  loss=1.0526  0.14s/batch  ETA=1.8m\n",
            "[2026-02-14 11:08:34]   train: batch 207/898  loss=0.9058  0.15s/batch  ETA=1.7m\n",
            "[2026-02-14 11:08:44]   train: batch 275/898  loss=1.5369  0.15s/batch  ETA=1.5m\n",
            "[2026-02-14 11:08:54]   train: batch 342/898  loss=1.0609  0.15s/batch  ETA=1.4m\n",
            "[2026-02-14 11:09:04]   train: batch 410/898  loss=1.0038  0.15s/batch  ETA=1.2m\n",
            "[2026-02-14 11:09:14]   train: batch 481/898  loss=0.9858  0.15s/batch  ETA=1.0m\n",
            "[2026-02-14 11:09:25]   train: batch 549/898  loss=1.4831  0.15s/batch  ETA=0.9m\n",
            "[2026-02-14 11:09:35]   train: batch 617/898  loss=1.0435  0.15s/batch  ETA=0.7m\n",
            "[2026-02-14 11:09:45]   train: batch 684/898  loss=0.7158  0.15s/batch  ETA=0.5m\n",
            "[2026-02-14 11:09:55]   train: batch 751/898  loss=1.5479  0.15s/batch  ETA=0.4m\n",
            "[2026-02-14 11:10:05]   train: batch 819/898  loss=1.4307  0.15s/batch  ETA=0.2m\n",
            "[2026-02-14 11:10:15]   train: batch 891/898  loss=1.4810  0.15s/batch  ETA=0.0m\n",
            "[2026-02-14 11:10:16] train Loss: 1.2637 Acc: 0.7429\n",
            "[2026-02-14 11:10:26]   test: batch 123/225  loss=1.5355  0.08s/batch  ETA=0.1m\n",
            "[2026-02-14 11:10:35] test Loss: 1.6474 Acc: 0.6350\n",
            "[2026-02-14 11:10:35] Epoch time: 2.51 min\n",
            "[2026-02-14 11:10:35] Epoch 10/30\n",
            "[2026-02-14 11:10:35] LR: 0.000256\n",
            "[2026-02-14 11:10:45]   train: batch 67/898  loss=1.3867  0.15s/batch  ETA=2.1m\n",
            "[2026-02-14 11:10:55]   train: batch 134/898  loss=1.3674  0.15s/batch  ETA=1.9m\n",
            "[2026-02-14 11:11:05]   train: batch 205/898  loss=1.0830  0.15s/batch  ETA=1.7m\n",
            "[2026-02-14 11:11:15]   train: batch 275/898  loss=1.4196  0.15s/batch  ETA=1.5m\n",
            "[2026-02-14 11:11:25]   train: batch 342/898  loss=1.2410  0.15s/batch  ETA=1.4m\n",
            "[2026-02-14 11:11:35]   train: batch 409/898  loss=1.3130  0.15s/batch  ETA=1.2m\n",
            "[2026-02-14 11:11:45]   train: batch 477/898  loss=0.8506  0.15s/batch  ETA=1.0m\n",
            "[2026-02-14 11:11:55]   train: batch 545/898  loss=0.8764  0.15s/batch  ETA=0.9m\n",
            "[2026-02-14 11:12:05]   train: batch 617/898  loss=1.0786  0.15s/batch  ETA=0.7m\n",
            "[2026-02-14 11:12:16]   train: batch 686/898  loss=1.6827  0.15s/batch  ETA=0.5m\n",
            "[2026-02-14 11:12:26]   train: batch 754/898  loss=1.0085  0.15s/batch  ETA=0.4m\n",
            "[2026-02-14 11:12:36]   train: batch 822/898  loss=0.9951  0.15s/batch  ETA=0.2m\n",
            "[2026-02-14 11:12:46]   train: batch 889/898  loss=1.2330  0.15s/batch  ETA=0.0m\n",
            "[2026-02-14 11:12:47] train Loss: 1.2180 Acc: 0.7678\n",
            "[2026-02-14 11:12:57]   test: batch 121/225  loss=1.7302  0.08s/batch  ETA=0.1m\n",
            "[2026-02-14 11:13:05] test Loss: 1.6526 Acc: 0.6516\n",
            "[2026-02-14 11:13:05] Epoch time: 2.50 min\n",
            "[2026-02-14 11:13:05] Epoch 11/30\n",
            "[2026-02-14 11:13:05] LR: 0.000244\n",
            "[2026-02-14 11:13:15]   train: batch 68/898  loss=0.8307  0.15s/batch  ETA=2.1m\n",
            "[2026-02-14 11:13:25]   train: batch 135/898  loss=1.2895  0.15s/batch  ETA=1.9m\n",
            "[2026-02-14 11:13:35]   train: batch 203/898  loss=1.5668  0.15s/batch  ETA=1.7m\n",
            "[2026-02-14 11:13:45]   train: batch 271/898  loss=1.1378  0.15s/batch  ETA=1.6m\n",
            "[2026-02-14 11:13:55]   train: batch 342/898  loss=0.8714  0.15s/batch  ETA=1.4m\n",
            "[2026-02-14 11:14:05]   train: batch 411/898  loss=1.3123  0.15s/batch  ETA=1.2m\n",
            "[2026-02-14 11:14:15]   train: batch 479/898  loss=0.8805  0.15s/batch  ETA=1.0m\n",
            "[2026-02-14 11:14:26]   train: batch 547/898  loss=1.3263  0.15s/batch  ETA=0.9m\n",
            "[2026-02-14 11:14:36]   train: batch 614/898  loss=1.2564  0.15s/batch  ETA=0.7m\n",
            "[2026-02-14 11:14:46]   train: batch 682/898  loss=1.2916  0.15s/batch  ETA=0.5m\n",
            "[2026-02-14 11:14:56]   train: batch 754/898  loss=1.1458  0.15s/batch  ETA=0.4m\n",
            "[2026-02-14 11:15:06]   train: batch 823/898  loss=1.7009  0.15s/batch  ETA=0.2m\n",
            "[2026-02-14 11:15:16]   train: batch 891/898  loss=1.4612  0.15s/batch  ETA=0.0m\n",
            "[2026-02-14 11:15:17] train Loss: 1.1779 Acc: 0.7913\n",
            "[2026-02-14 11:15:27]   test: batch 123/225  loss=1.6736  0.08s/batch  ETA=0.1m\n",
            "[2026-02-14 11:15:35] test Loss: 1.6437 Acc: 0.6525\n",
            "[2026-02-14 11:15:35] Epoch time: 2.50 min\n",
            "[2026-02-14 11:15:35] Epoch 12/30\n",
            "[2026-02-14 11:15:35] LR: 0.000230\n",
            "[2026-02-14 11:15:45]   train: batch 70/898  loss=1.1023  0.14s/batch  ETA=2.0m\n",
            "[2026-02-14 11:15:55]   train: batch 139/898  loss=1.2756  0.14s/batch  ETA=1.8m\n",
            "[2026-02-14 11:16:05]   train: batch 206/898  loss=1.4038  0.15s/batch  ETA=1.7m\n",
            "[2026-02-14 11:16:15]   train: batch 273/898  loss=1.0777  0.15s/batch  ETA=1.5m\n",
            "[2026-02-14 11:16:25]   train: batch 340/898  loss=1.4412  0.15s/batch  ETA=1.4m\n",
            "[2026-02-14 11:16:36]   train: batch 408/898  loss=1.1981  0.15s/batch  ETA=1.2m\n",
            "[2026-02-14 11:16:46]   train: batch 478/898  loss=1.2131  0.15s/batch  ETA=1.0m\n",
            "[2026-02-14 11:16:56]   train: batch 546/898  loss=1.1488  0.15s/batch  ETA=0.9m\n",
            "[2026-02-14 11:17:06]   train: batch 614/898  loss=1.2778  0.15s/batch  ETA=0.7m\n",
            "[2026-02-14 11:17:16]   train: batch 682/898  loss=1.2141  0.15s/batch  ETA=0.5m\n",
            "[2026-02-14 11:17:26]   train: batch 749/898  loss=1.2084  0.15s/batch  ETA=0.4m\n",
            "[2026-02-14 11:17:36]   train: batch 817/898  loss=1.2357  0.15s/batch  ETA=0.2m\n",
            "[2026-02-14 11:17:46]   train: batch 889/898  loss=1.2109  0.15s/batch  ETA=0.0m\n",
            "[2026-02-14 11:17:48] train Loss: 1.1348 Acc: 0.8137\n",
            "[2026-02-14 11:17:58]   test: batch 127/225  loss=1.6839  0.08s/batch  ETA=0.1m\n",
            "[2026-02-14 11:18:06] test Loss: 1.6925 Acc: 0.6429\n",
            "[2026-02-14 11:18:06] Epoch time: 2.51 min\n",
            "[2026-02-14 11:18:06] Epoch 13/30\n",
            "[2026-02-14 11:18:06] LR: 0.000215\n",
            "[2026-02-14 11:18:16]   train: batch 68/898  loss=0.7187  0.15s/batch  ETA=2.1m\n",
            "[2026-02-14 11:18:26]   train: batch 135/898  loss=1.3625  0.15s/batch  ETA=1.9m\n",
            "[2026-02-14 11:18:36]   train: batch 204/898  loss=0.9910  0.15s/batch  ETA=1.7m\n",
            "[2026-02-14 11:18:46]   train: batch 276/898  loss=1.3698  0.15s/batch  ETA=1.5m\n",
            "[2026-02-14 11:18:56]   train: batch 344/898  loss=1.1860  0.15s/batch  ETA=1.4m\n",
            "[2026-02-14 11:19:06]   train: batch 412/898  loss=1.1305  0.15s/batch  ETA=1.2m\n",
            "[2026-02-14 11:19:16]   train: batch 480/898  loss=1.2258  0.15s/batch  ETA=1.0m\n",
            "[2026-02-14 11:19:26]   train: batch 548/898  loss=1.2327  0.15s/batch  ETA=0.9m\n",
            "[2026-02-14 11:19:37]   train: batch 617/898  loss=1.4496  0.15s/batch  ETA=0.7m\n",
            "[2026-02-14 11:19:47]   train: batch 687/898  loss=1.4085  0.15s/batch  ETA=0.5m\n",
            "[2026-02-14 11:19:57]   train: batch 755/898  loss=1.1728  0.15s/batch  ETA=0.4m\n",
            "[2026-02-14 11:20:07]   train: batch 823/898  loss=1.2229  0.15s/batch  ETA=0.2m\n",
            "[2026-02-14 11:20:17]   train: batch 890/898  loss=1.0892  0.15s/batch  ETA=0.0m\n",
            "[2026-02-14 11:20:18] train Loss: 1.1040 Acc: 0.8311\n",
            "[2026-02-14 11:20:28]   test: batch 122/225  loss=1.5120  0.08s/batch  ETA=0.1m\n",
            "[2026-02-14 11:20:36] test Loss: 1.6560 Acc: 0.6609\n",
            "[2026-02-14 11:20:36] New best test acc: 0.6609 -> saved wafflelover404_emotion_model_best.pth\n",
            "[2026-02-14 11:20:36] Epoch time: 2.50 min\n",
            "[2026-02-14 11:20:36] Epoch 14/30\n",
            "[2026-02-14 11:20:36] LR: 0.000200\n",
            "[2026-02-14 11:20:46]   train: batch 67/898  loss=1.2114  0.15s/batch  ETA=2.1m\n",
            "[2026-02-14 11:20:56]   train: batch 135/898  loss=1.1730  0.15s/batch  ETA=1.9m\n",
            "[2026-02-14 11:21:06]   train: batch 201/898  loss=0.9744  0.15s/batch  ETA=1.7m\n",
            "[2026-02-14 11:21:16]   train: batch 267/898  loss=1.1452  0.15s/batch  ETA=1.6m\n",
            "[2026-02-14 11:21:26]   train: batch 335/898  loss=1.2307  0.15s/batch  ETA=1.4m\n",
            "[2026-02-14 11:21:36]   train: batch 406/898  loss=1.1485  0.15s/batch  ETA=1.2m\n",
            "[2026-02-14 11:21:46]   train: batch 472/898  loss=1.3215  0.15s/batch  ETA=1.1m\n",
            "[2026-02-14 11:21:57]   train: batch 539/898  loss=1.1191  0.15s/batch  ETA=0.9m\n",
            "[2026-02-14 11:22:07]   train: batch 606/898  loss=1.1772  0.15s/batch  ETA=0.7m\n",
            "[2026-02-14 11:22:17]   train: batch 673/898  loss=1.1124  0.15s/batch  ETA=0.6m\n",
            "[2026-02-14 11:22:27]   train: batch 741/898  loss=0.7925  0.15s/batch  ETA=0.4m\n",
            "[2026-02-14 11:22:37]   train: batch 811/898  loss=0.5317  0.15s/batch  ETA=0.2m\n",
            "[2026-02-14 11:22:47]   train: batch 877/898  loss=1.1402  0.15s/batch  ETA=0.1m\n",
            "[2026-02-14 11:22:50] train Loss: 1.0696 Acc: 0.8511\n",
            "[2026-02-14 11:23:00]   test: batch 120/225  loss=2.0220  0.08s/batch  ETA=0.1m\n",
            "[2026-02-14 11:23:09] test Loss: 1.7088 Acc: 0.6432\n",
            "[2026-02-14 11:23:09] Epoch time: 2.55 min\n",
            "[2026-02-14 11:23:09] Epoch 15/30\n",
            "[2026-02-14 11:23:09] LR: 0.000183\n",
            "[2026-02-14 11:23:19]   train: batch 67/898  loss=1.0058  0.15s/batch  ETA=2.1m\n",
            "[2026-02-14 11:23:29]   train: batch 134/898  loss=1.1504  0.15s/batch  ETA=1.9m\n",
            "[2026-02-14 11:23:39]   train: batch 202/898  loss=0.7740  0.15s/batch  ETA=1.7m\n",
            "[2026-02-14 11:23:49]   train: batch 270/898  loss=1.1251  0.15s/batch  ETA=1.6m\n",
            "[2026-02-14 11:23:59]   train: batch 336/898  loss=1.2339  0.15s/batch  ETA=1.4m\n",
            "[2026-02-14 11:24:09]   train: batch 402/898  loss=0.9882  0.15s/batch  ETA=1.2m\n",
            "[2026-02-14 11:24:19]   train: batch 468/898  loss=1.2019  0.15s/batch  ETA=1.1m\n",
            "[2026-02-14 11:24:29]   train: batch 535/898  loss=1.0036  0.15s/batch  ETA=0.9m\n",
            "[2026-02-14 11:24:39]   train: batch 606/898  loss=1.0295  0.15s/batch  ETA=0.7m\n",
            "[2026-02-14 11:24:50]   train: batch 673/898  loss=0.9061  0.15s/batch  ETA=0.6m\n",
            "[2026-02-14 11:25:00]   train: batch 739/898  loss=0.9521  0.15s/batch  ETA=0.4m\n",
            "[2026-02-14 11:25:10]   train: batch 805/898  loss=1.3037  0.15s/batch  ETA=0.2m\n",
            "[2026-02-14 11:25:20]   train: batch 872/898  loss=1.3783  0.15s/batch  ETA=0.1m\n",
            "[2026-02-14 11:25:23] train Loss: 1.0365 Acc: 0.8686\n",
            "[2026-02-14 11:25:33]   test: batch 123/225  loss=1.9492  0.08s/batch  ETA=0.1m\n",
            "[2026-02-14 11:25:41] test Loss: 1.6819 Acc: 0.6623\n",
            "[2026-02-14 11:25:42] New best test acc: 0.6623 -> saved wafflelover404_emotion_model_best.pth\n",
            "[2026-02-14 11:25:42] Epoch time: 2.55 min\n",
            "[2026-02-14 11:25:42] Epoch 16/30\n",
            "[2026-02-14 11:25:42] LR: 0.000167\n",
            "[2026-02-14 11:25:52]   train: batch 71/898  loss=0.7202  0.14s/batch  ETA=1.9m\n",
            "[2026-02-14 11:26:02]   train: batch 139/898  loss=1.0442  0.14s/batch  ETA=1.8m\n",
            "[2026-02-14 11:26:12]   train: batch 206/898  loss=0.7591  0.15s/batch  ETA=1.7m\n",
            "[2026-02-14 11:26:22]   train: batch 272/898  loss=1.0835  0.15s/batch  ETA=1.5m\n",
            "[2026-02-14 11:26:32]   train: batch 338/898  loss=0.8927  0.15s/batch  ETA=1.4m\n",
            "[2026-02-14 11:26:42]   train: batch 407/898  loss=1.2376  0.15s/batch  ETA=1.2m\n",
            "[2026-02-14 11:26:52]   train: batch 476/898  loss=1.0100  0.15s/batch  ETA=1.0m\n",
            "[2026-02-14 11:27:02]   train: batch 543/898  loss=1.1007  0.15s/batch  ETA=0.9m\n",
            "[2026-02-14 11:27:12]   train: batch 610/898  loss=1.0912  0.15s/batch  ETA=0.7m\n",
            "[2026-02-14 11:27:23]   train: batch 677/898  loss=1.0975  0.15s/batch  ETA=0.5m\n",
            "[2026-02-14 11:27:33]   train: batch 743/898  loss=1.2728  0.15s/batch  ETA=0.4m\n",
            "[2026-02-14 11:27:43]   train: batch 813/898  loss=1.2862  0.15s/batch  ETA=0.2m\n",
            "[2026-02-14 11:27:53]   train: batch 881/898  loss=1.0342  0.15s/batch  ETA=0.0m\n",
            "[2026-02-14 11:27:55] train Loss: 1.0137 Acc: 0.8813\n",
            "[2026-02-14 11:28:05]   test: batch 121/225  loss=1.7698  0.08s/batch  ETA=0.1m\n",
            "[2026-02-14 11:28:14] test Loss: 1.7042 Acc: 0.6523\n",
            "[2026-02-14 11:28:14] Epoch time: 2.54 min\n",
            "[2026-02-14 11:28:14] Epoch 17/30\n",
            "[2026-02-14 11:28:14] LR: 0.000150\n",
            "[2026-02-14 11:28:24]   train: batch 67/898  loss=1.2417  0.15s/batch  ETA=2.1m\n",
            "[2026-02-14 11:28:34]   train: batch 134/898  loss=1.2075  0.15s/batch  ETA=1.9m\n",
            "[2026-02-14 11:28:45]   train: batch 203/898  loss=0.9595  0.15s/batch  ETA=1.7m\n",
            "[2026-02-14 11:28:55]   train: batch 274/898  loss=0.8646  0.15s/batch  ETA=1.5m\n",
            "[2026-02-14 11:29:05]   train: batch 340/898  loss=0.7702  0.15s/batch  ETA=1.4m\n",
            "[2026-02-14 11:29:15]   train: batch 406/898  loss=0.7811  0.15s/batch  ETA=1.2m\n",
            "[2026-02-14 11:29:25]   train: batch 471/898  loss=1.0800  0.15s/batch  ETA=1.1m\n",
            "[2026-02-14 11:29:35]   train: batch 537/898  loss=1.2622  0.15s/batch  ETA=0.9m\n",
            "[2026-02-14 11:29:45]   train: batch 605/898  loss=1.0494  0.15s/batch  ETA=0.7m\n",
            "[2026-02-14 11:29:55]   train: batch 672/898  loss=1.1514  0.15s/batch  ETA=0.6m\n",
            "[2026-02-14 11:30:05]   train: batch 737/898  loss=1.2195  0.15s/batch  ETA=0.4m\n",
            "[2026-02-14 11:30:15]   train: batch 804/898  loss=1.3145  0.15s/batch  ETA=0.2m\n",
            "[2026-02-14 11:30:25]   train: batch 870/898  loss=1.0643  0.15s/batch  ETA=0.1m\n",
            "[2026-02-14 11:30:29] train Loss: 0.9879 Acc: 0.8960\n",
            "[2026-02-14 11:30:39]   test: batch 123/225  loss=1.5334  0.08s/batch  ETA=0.1m\n",
            "[2026-02-14 11:30:47] test Loss: 1.7284 Acc: 0.6546\n",
            "[2026-02-14 11:30:47] Epoch time: 2.55 min\n",
            "[2026-02-14 11:30:47] Epoch 18/30\n",
            "[2026-02-14 11:30:47] LR: 0.000133\n",
            "[2026-02-14 11:30:57]   train: batch 71/898  loss=0.7546  0.14s/batch  ETA=2.0m\n",
            "[2026-02-14 11:31:08]   train: batch 140/898  loss=1.0929  0.14s/batch  ETA=1.8m\n",
            "[2026-02-14 11:31:18]   train: batch 206/898  loss=1.0385  0.15s/batch  ETA=1.7m\n",
            "[2026-02-14 11:31:28]   train: batch 272/898  loss=1.1123  0.15s/batch  ETA=1.6m\n",
            "[2026-02-14 11:31:38]   train: batch 336/898  loss=0.9379  0.15s/batch  ETA=1.4m\n",
            "[2026-02-14 11:31:48]   train: batch 402/898  loss=1.1640  0.15s/batch  ETA=1.2m\n",
            "[2026-02-14 11:31:58]   train: batch 473/898  loss=1.1855  0.15s/batch  ETA=1.1m\n",
            "[2026-02-14 11:32:08]   train: batch 540/898  loss=0.8992  0.15s/batch  ETA=0.9m\n",
            "[2026-02-14 11:32:18]   train: batch 606/898  loss=1.0525  0.15s/batch  ETA=0.7m\n",
            "[2026-02-14 11:32:28]   train: batch 673/898  loss=1.0989  0.15s/batch  ETA=0.6m\n",
            "[2026-02-14 11:32:38]   train: batch 739/898  loss=0.6323  0.15s/batch  ETA=0.4m\n",
            "[2026-02-14 11:32:48]   train: batch 806/898  loss=0.8083  0.15s/batch  ETA=0.2m\n",
            "[2026-02-14 11:32:58]   train: batch 877/898  loss=0.8863  0.15s/batch  ETA=0.1m\n",
            "[2026-02-14 11:33:02] train Loss: 0.9666 Acc: 0.9083\n",
            "[2026-02-14 11:33:12]   test: batch 124/225  loss=1.3081  0.08s/batch  ETA=0.1m\n",
            "[2026-02-14 11:33:20] test Loss: 1.7081 Acc: 0.6616\n",
            "[2026-02-14 11:33:20] Epoch time: 2.54 min\n",
            "[2026-02-14 11:33:20] Epoch 19/30\n",
            "[2026-02-14 11:33:20] LR: 0.000117\n",
            "[2026-02-14 11:33:30]   train: batch 67/898  loss=0.9886  0.15s/batch  ETA=2.1m\n",
            "[2026-02-14 11:33:40]   train: batch 133/898  loss=1.0703  0.15s/batch  ETA=1.9m\n",
            "[2026-02-14 11:33:50]   train: batch 200/898  loss=0.9795  0.15s/batch  ETA=1.8m\n",
            "[2026-02-14 11:34:00]   train: batch 267/898  loss=0.7240  0.15s/batch  ETA=1.6m\n",
            "[2026-02-14 11:34:10]   train: batch 336/898  loss=0.9770  0.15s/batch  ETA=1.4m\n",
            "[2026-02-14 11:34:20]   train: batch 402/898  loss=0.6454  0.15s/batch  ETA=1.2m\n",
            "[2026-02-14 11:34:31]   train: batch 469/898  loss=0.6478  0.15s/batch  ETA=1.1m\n",
            "[2026-02-14 11:34:41]   train: batch 535/898  loss=1.2024  0.15s/batch  ETA=0.9m\n",
            "[2026-02-14 11:34:51]   train: batch 603/898  loss=1.0896  0.15s/batch  ETA=0.7m\n",
            "[2026-02-14 11:35:01]   train: batch 672/898  loss=1.0889  0.15s/batch  ETA=0.6m\n",
            "[2026-02-14 11:35:11]   train: batch 739/898  loss=0.7657  0.15s/batch  ETA=0.4m\n",
            "[2026-02-14 11:35:21]   train: batch 803/898  loss=0.9986  0.15s/batch  ETA=0.2m\n",
            "[2026-02-14 11:35:31]   train: batch 870/898  loss=1.0526  0.15s/batch  ETA=0.1m\n",
            "[2026-02-14 11:35:35] train Loss: 0.9453 Acc: 0.9204\n",
            "[2026-02-14 11:35:45]   test: batch 124/225  loss=1.2643  0.08s/batch  ETA=0.1m\n",
            "[2026-02-14 11:35:53] test Loss: 1.7134 Acc: 0.6636\n",
            "[2026-02-14 11:35:53] New best test acc: 0.6636 -> saved wafflelover404_emotion_model_best.pth\n",
            "[2026-02-14 11:35:53] Epoch time: 2.55 min\n",
            "[2026-02-14 11:35:53] Epoch 20/30\n",
            "[2026-02-14 11:35:53] LR: 0.000100\n",
            "[2026-02-14 11:36:03]   train: batch 68/898  loss=0.7509  0.15s/batch  ETA=2.1m\n",
            "[2026-02-14 11:36:13]   train: batch 138/898  loss=1.0798  0.15s/batch  ETA=1.9m\n",
            "[2026-02-14 11:36:23]   train: batch 206/898  loss=0.5805  0.15s/batch  ETA=1.7m\n",
            "[2026-02-14 11:36:33]   train: batch 273/898  loss=0.7332  0.15s/batch  ETA=1.5m\n",
            "[2026-02-14 11:36:43]   train: batch 338/898  loss=1.0377  0.15s/batch  ETA=1.4m\n",
            "[2026-02-14 11:36:53]   train: batch 404/898  loss=0.7273  0.15s/batch  ETA=1.2m\n",
            "[2026-02-14 11:37:03]   train: batch 471/898  loss=1.1641  0.15s/batch  ETA=1.1m\n",
            "[2026-02-14 11:37:13]   train: batch 540/898  loss=1.1880  0.15s/batch  ETA=0.9m\n",
            "[2026-02-14 11:37:24]   train: batch 606/898  loss=1.0140  0.15s/batch  ETA=0.7m\n",
            "[2026-02-14 11:37:34]   train: batch 672/898  loss=0.9057  0.15s/batch  ETA=0.6m\n",
            "[2026-02-14 11:37:44]   train: batch 738/898  loss=0.8088  0.15s/batch  ETA=0.4m\n",
            "[2026-02-14 11:37:54]   train: batch 805/898  loss=0.7630  0.15s/batch  ETA=0.2m\n",
            "[2026-02-14 11:38:04]   train: batch 872/898  loss=0.9482  0.15s/batch  ETA=0.1m\n",
            "[2026-02-14 11:38:07] train Loss: 0.9265 Acc: 0.9294\n",
            "[2026-02-14 11:38:17]   test: batch 121/225  loss=1.6562  0.08s/batch  ETA=0.1m\n",
            "[2026-02-14 11:38:26] test Loss: 1.6951 Acc: 0.6640\n",
            "[2026-02-14 11:38:26] New best test acc: 0.6640 -> saved wafflelover404_emotion_model_best.pth\n",
            "[2026-02-14 11:38:26] Epoch time: 2.55 min\n",
            "[2026-02-14 11:38:26] Epoch 21/30\n",
            "[2026-02-14 11:38:26] LR: 0.000085\n",
            "[2026-02-14 11:38:36]   train: batch 66/898  loss=0.5330  0.15s/batch  ETA=2.1m\n",
            "[2026-02-14 11:38:46]   train: batch 131/898  loss=0.7472  0.15s/batch  ETA=2.0m\n",
            "[2026-02-14 11:38:56]   train: batch 197/898  loss=0.8920  0.15s/batch  ETA=1.8m\n",
            "[2026-02-14 11:39:06]   train: batch 263/898  loss=1.1600  0.15s/batch  ETA=1.6m\n",
            "[2026-02-14 11:39:16]   train: batch 332/898  loss=1.0053  0.15s/batch  ETA=1.4m\n",
            "[2026-02-14 11:39:26]   train: batch 402/898  loss=0.7595  0.15s/batch  ETA=1.2m\n",
            "[2026-02-14 11:39:36]   train: batch 468/898  loss=0.8003  0.15s/batch  ETA=1.1m\n",
            "[2026-02-14 11:39:47]   train: batch 534/898  loss=0.9863  0.15s/batch  ETA=0.9m\n",
            "[2026-02-14 11:39:57]   train: batch 600/898  loss=0.8557  0.15s/batch  ETA=0.8m\n",
            "[2026-02-14 11:40:07]   train: batch 667/898  loss=1.0311  0.15s/batch  ETA=0.6m\n",
            "[2026-02-14 11:40:17]   train: batch 737/898  loss=0.7805  0.15s/batch  ETA=0.4m\n",
            "[2026-02-14 11:40:27]   train: batch 806/898  loss=0.9908  0.15s/batch  ETA=0.2m\n",
            "[2026-02-14 11:40:37]   train: batch 872/898  loss=0.9698  0.15s/batch  ETA=0.1m\n",
            "[2026-02-14 11:40:41] train Loss: 0.9084 Acc: 0.9393\n",
            "[2026-02-14 11:40:51]   test: batch 122/225  loss=1.4273  0.08s/batch  ETA=0.1m\n",
            "[2026-02-14 11:40:59] test Loss: 1.7032 Acc: 0.6670\n",
            "[2026-02-14 11:40:59] New best test acc: 0.6670 -> saved wafflelover404_emotion_model_best.pth\n",
            "[2026-02-14 11:40:59] Epoch time: 2.56 min\n",
            "[2026-02-14 11:40:59] Epoch 22/30\n",
            "[2026-02-14 11:40:59] LR: 0.000070\n",
            "[2026-02-14 11:41:09]   train: batch 66/898  loss=0.7406  0.15s/batch  ETA=2.1m\n",
            "[2026-02-14 11:41:20]   train: batch 133/898  loss=1.0255  0.15s/batch  ETA=1.9m\n",
            "[2026-02-14 11:41:30]   train: batch 203/898  loss=1.0544  0.15s/batch  ETA=1.7m\n",
            "[2026-02-14 11:41:40]   train: batch 271/898  loss=0.9667  0.15s/batch  ETA=1.6m\n",
            "[2026-02-14 11:41:50]   train: batch 338/898  loss=0.9672  0.15s/batch  ETA=1.4m\n",
            "[2026-02-14 11:42:00]   train: batch 404/898  loss=1.0474  0.15s/batch  ETA=1.2m\n",
            "[2026-02-14 11:42:10]   train: batch 470/898  loss=0.7652  0.15s/batch  ETA=1.1m\n",
            "[2026-02-14 11:42:20]   train: batch 537/898  loss=1.0829  0.15s/batch  ETA=0.9m\n",
            "[2026-02-14 11:42:30]   train: batch 608/898  loss=1.0342  0.15s/batch  ETA=0.7m\n",
            "[2026-02-14 11:42:40]   train: batch 674/898  loss=1.0256  0.15s/batch  ETA=0.6m\n",
            "[2026-02-14 11:42:50]   train: batch 740/898  loss=0.9398  0.15s/batch  ETA=0.4m\n",
            "[2026-02-14 11:43:00]   train: batch 806/898  loss=1.0551  0.15s/batch  ETA=0.2m\n",
            "[2026-02-14 11:43:10]   train: batch 872/898  loss=0.7633  0.15s/batch  ETA=0.1m\n",
            "[2026-02-14 11:43:14] train Loss: 0.8973 Acc: 0.9448\n",
            "[2026-02-14 11:43:24]   test: batch 119/225  loss=1.7664  0.08s/batch  ETA=0.1m\n",
            "[2026-02-14 11:43:33] test Loss: 1.7118 Acc: 0.6608\n",
            "[2026-02-14 11:43:33] Epoch time: 2.56 min\n",
            "[2026-02-14 11:43:33] Epoch 23/30\n",
            "[2026-02-14 11:43:33] LR: 0.000056\n",
            "[2026-02-14 11:43:43]   train: batch 70/898  loss=0.9638  0.14s/batch  ETA=2.0m\n",
            "[2026-02-14 11:43:53]   train: batch 135/898  loss=0.8941  0.15s/batch  ETA=1.9m\n",
            "[2026-02-14 11:44:03]   train: batch 201/898  loss=0.5751  0.15s/batch  ETA=1.7m\n",
            "[2026-02-14 11:44:13]   train: batch 267/898  loss=0.9441  0.15s/batch  ETA=1.6m\n",
            "[2026-02-14 11:44:23]   train: batch 333/898  loss=0.9812  0.15s/batch  ETA=1.4m\n",
            "[2026-02-14 11:44:33]   train: batch 402/898  loss=0.8859  0.15s/batch  ETA=1.2m\n",
            "[2026-02-14 11:44:43]   train: batch 468/898  loss=1.0609  0.15s/batch  ETA=1.1m\n",
            "[2026-02-14 11:44:53]   train: batch 534/898  loss=0.9396  0.15s/batch  ETA=0.9m\n",
            "[2026-02-14 11:45:04]   train: batch 601/898  loss=1.0332  0.15s/batch  ETA=0.7m\n",
            "[2026-02-14 11:45:14]   train: batch 667/898  loss=0.6830  0.15s/batch  ETA=0.6m\n",
            "[2026-02-14 11:45:24]   train: batch 731/898  loss=0.9395  0.15s/batch  ETA=0.4m\n",
            "[2026-02-14 11:45:34]   train: batch 801/898  loss=0.5962  0.15s/batch  ETA=0.2m\n",
            "[2026-02-14 11:45:44]   train: batch 866/898  loss=0.9691  0.15s/batch  ETA=0.1m\n",
            "[2026-02-14 11:45:49] train Loss: 0.8860 Acc: 0.9524\n",
            "[2026-02-14 11:45:59]   test: batch 122/225  loss=1.3779  0.08s/batch  ETA=0.1m\n",
            "[2026-02-14 11:46:08] test Loss: 1.6994 Acc: 0.6675\n",
            "[2026-02-14 11:46:08] New best test acc: 0.6675 -> saved wafflelover404_emotion_model_best.pth\n",
            "[2026-02-14 11:46:08] Epoch time: 2.58 min\n",
            "[2026-02-14 11:46:08] Epoch 24/30\n",
            "[2026-02-14 11:46:08] LR: 0.000044\n",
            "[2026-02-14 11:46:18]   train: batch 66/898  loss=0.9881  0.15s/batch  ETA=2.1m\n",
            "[2026-02-14 11:46:28]   train: batch 133/898  loss=0.9987  0.15s/batch  ETA=1.9m\n",
            "[2026-02-14 11:46:38]   train: batch 199/898  loss=0.9431  0.15s/batch  ETA=1.8m\n",
            "[2026-02-14 11:46:48]   train: batch 266/898  loss=0.8732  0.15s/batch  ETA=1.6m\n",
            "[2026-02-14 11:46:58]   train: batch 336/898  loss=0.9374  0.15s/batch  ETA=1.4m\n",
            "[2026-02-14 11:47:08]   train: batch 401/898  loss=0.6840  0.15s/batch  ETA=1.2m\n",
            "[2026-02-14 11:47:18]   train: batch 467/898  loss=0.7622  0.15s/batch  ETA=1.1m\n",
            "[2026-02-14 11:47:28]   train: batch 533/898  loss=0.6530  0.15s/batch  ETA=0.9m\n",
            "[2026-02-14 11:47:38]   train: batch 599/898  loss=0.9676  0.15s/batch  ETA=0.8m\n",
            "[2026-02-14 11:47:48]   train: batch 666/898  loss=0.7556  0.15s/batch  ETA=0.6m\n",
            "[2026-02-14 11:47:58]   train: batch 737/898  loss=0.9404  0.15s/batch  ETA=0.4m\n",
            "[2026-02-14 11:48:08]   train: batch 804/898  loss=0.9956  0.15s/batch  ETA=0.2m\n",
            "[2026-02-14 11:48:19]   train: batch 870/898  loss=0.9500  0.15s/batch  ETA=0.1m\n",
            "[2026-02-14 11:48:22] train Loss: 0.8711 Acc: 0.9564\n",
            "[2026-02-14 11:48:32]   test: batch 121/225  loss=1.6389  0.08s/batch  ETA=0.1m\n",
            "[2026-02-14 11:48:41] test Loss: 1.6998 Acc: 0.6679\n",
            "[2026-02-14 11:48:41] New best test acc: 0.6679 -> saved wafflelover404_emotion_model_best.pth\n",
            "[2026-02-14 11:48:41] Epoch time: 2.56 min\n",
            "[2026-02-14 11:48:41] Epoch 25/30\n",
            "[2026-02-14 11:48:41] LR: 0.000033\n",
            "[2026-02-14 11:48:52]   train: batch 66/898  loss=0.9805  0.15s/batch  ETA=2.1m\n",
            "[2026-02-14 11:49:02]   train: batch 133/898  loss=0.8813  0.15s/batch  ETA=1.9m\n",
            "[2026-02-14 11:49:12]   train: batch 202/898  loss=0.5778  0.15s/batch  ETA=1.7m\n",
            "[2026-02-14 11:49:22]   train: batch 271/898  loss=1.1678  0.15s/batch  ETA=1.6m\n",
            "[2026-02-14 11:49:32]   train: batch 339/898  loss=0.7283  0.15s/batch  ETA=1.4m\n",
            "[2026-02-14 11:49:42]   train: batch 404/898  loss=0.7327  0.15s/batch  ETA=1.2m\n",
            "[2026-02-14 11:49:52]   train: batch 470/898  loss=0.6986  0.15s/batch  ETA=1.1m\n",
            "[2026-02-14 11:50:02]   train: batch 537/898  loss=0.7116  0.15s/batch  ETA=0.9m\n",
            "[2026-02-14 11:50:12]   train: batch 604/898  loss=0.6315  0.15s/batch  ETA=0.7m\n",
            "[2026-02-14 11:50:22]   train: batch 673/898  loss=1.0060  0.15s/batch  ETA=0.6m\n",
            "[2026-02-14 11:50:33]   train: batch 740/898  loss=0.9713  0.15s/batch  ETA=0.4m\n",
            "[2026-02-14 11:50:43]   train: batch 806/898  loss=0.6884  0.15s/batch  ETA=0.2m\n",
            "[2026-02-14 11:50:53]   train: batch 871/898  loss=0.9166  0.15s/batch  ETA=0.1m\n",
            "[2026-02-14 11:50:56] train Loss: 0.8686 Acc: 0.9596\n",
            "[2026-02-14 11:51:06]   test: batch 121/225  loss=1.6956  0.08s/batch  ETA=0.1m\n",
            "[2026-02-14 11:51:15] test Loss: 1.6962 Acc: 0.6663\n",
            "[2026-02-14 11:51:15] Epoch time: 2.56 min\n",
            "[2026-02-14 11:51:15] Epoch 26/30\n",
            "[2026-02-14 11:51:15] LR: 0.000023\n",
            "[2026-02-14 11:51:25]   train: batch 67/898  loss=0.7409  0.15s/batch  ETA=2.1m\n",
            "[2026-02-14 11:51:35]   train: batch 138/898  loss=0.9749  0.15s/batch  ETA=1.9m\n",
            "[2026-02-14 11:51:45]   train: batch 204/898  loss=0.9191  0.15s/batch  ETA=1.7m\n",
            "[2026-02-14 11:51:56]   train: batch 271/898  loss=0.9830  0.15s/batch  ETA=1.6m\n",
            "[2026-02-14 11:52:06]   train: batch 338/898  loss=1.0653  0.15s/batch  ETA=1.4m\n",
            "[2026-02-14 11:52:16]   train: batch 404/898  loss=0.9480  0.15s/batch  ETA=1.2m\n",
            "[2026-02-14 11:52:26]   train: batch 470/898  loss=0.7056  0.15s/batch  ETA=1.1m\n",
            "[2026-02-14 11:52:36]   train: batch 539/898  loss=1.0660  0.15s/batch  ETA=0.9m\n",
            "[2026-02-14 11:52:46]   train: batch 607/898  loss=1.0360  0.15s/batch  ETA=0.7m\n",
            "[2026-02-14 11:52:56]   train: batch 674/898  loss=0.7633  0.15s/batch  ETA=0.6m\n",
            "[2026-02-14 11:53:06]   train: batch 740/898  loss=1.0189  0.15s/batch  ETA=0.4m\n",
            "[2026-02-14 11:53:16]   train: batch 806/898  loss=0.9884  0.15s/batch  ETA=0.2m\n",
            "[2026-02-14 11:53:26]   train: batch 873/898  loss=0.9827  0.15s/batch  ETA=0.1m\n",
            "[2026-02-14 11:53:30] train Loss: 0.8594 Acc: 0.9639\n",
            "[2026-02-14 11:53:40]   test: batch 123/225  loss=1.7948  0.08s/batch  ETA=0.1m\n",
            "[2026-02-14 11:53:48] test Loss: 1.6980 Acc: 0.6690\n",
            "[2026-02-14 11:53:48] New best test acc: 0.6690 -> saved wafflelover404_emotion_model_best.pth\n",
            "[2026-02-14 11:53:48] Epoch time: 2.54 min\n",
            "[2026-02-14 11:53:48] Epoch 27/30\n",
            "[2026-02-14 11:53:48] LR: 0.000015\n",
            "[2026-02-14 11:53:58]   train: batch 67/898  loss=0.9101  0.15s/batch  ETA=2.1m\n",
            "[2026-02-14 11:54:08]   train: batch 133/898  loss=0.9602  0.15s/batch  ETA=1.9m\n",
            "[2026-02-14 11:54:18]   train: batch 200/898  loss=0.9430  0.15s/batch  ETA=1.8m\n",
            "[2026-02-14 11:54:28]   train: batch 267/898  loss=0.6491  0.15s/batch  ETA=1.6m\n",
            "[2026-02-14 11:54:38]   train: batch 335/898  loss=0.7363  0.15s/batch  ETA=1.4m\n",
            "[2026-02-14 11:54:48]   train: batch 405/898  loss=0.7074  0.15s/batch  ETA=1.2m\n",
            "[2026-02-14 11:54:58]   train: batch 473/898  loss=0.8589  0.15s/batch  ETA=1.1m\n",
            "[2026-02-14 11:55:08]   train: batch 541/898  loss=1.0353  0.15s/batch  ETA=0.9m\n",
            "[2026-02-14 11:55:18]   train: batch 608/898  loss=0.9209  0.15s/batch  ETA=0.7m\n",
            "[2026-02-14 11:55:28]   train: batch 675/898  loss=0.9507  0.15s/batch  ETA=0.6m\n",
            "[2026-02-14 11:55:38]   train: batch 743/898  loss=0.9904  0.15s/batch  ETA=0.4m\n",
            "[2026-02-14 11:55:48]   train: batch 813/898  loss=0.6782  0.15s/batch  ETA=0.2m\n",
            "[2026-02-14 11:55:58]   train: batch 882/898  loss=0.9490  0.15s/batch  ETA=0.0m\n",
            "[2026-02-14 11:56:01] train Loss: 0.8552 Acc: 0.9668\n",
            "[2026-02-14 11:56:11]   test: batch 126/225  loss=1.9957  0.08s/batch  ETA=0.1m\n",
            "[2026-02-14 11:56:19] test Loss: 1.6880 Acc: 0.6684\n",
            "[2026-02-14 11:56:19] Epoch time: 2.52 min\n",
            "[2026-02-14 11:56:19] Epoch 28/30\n",
            "[2026-02-14 11:56:19] LR: 0.000008\n",
            "[2026-02-14 11:56:29]   train: batch 67/898  loss=0.7769  0.15s/batch  ETA=2.1m\n",
            "[2026-02-14 11:56:39]   train: batch 134/898  loss=0.9209  0.15s/batch  ETA=1.9m\n",
            "[2026-02-14 11:56:49]   train: batch 203/898  loss=0.7039  0.15s/batch  ETA=1.7m\n",
            "[2026-02-14 11:56:59]   train: batch 274/898  loss=0.7166  0.15s/batch  ETA=1.5m\n",
            "[2026-02-14 11:57:09]   train: batch 342/898  loss=0.5711  0.15s/batch  ETA=1.4m\n",
            "[2026-02-14 11:57:19]   train: batch 410/898  loss=1.0228  0.15s/batch  ETA=1.2m\n",
            "[2026-02-14 11:57:30]   train: batch 477/898  loss=0.6896  0.15s/batch  ETA=1.0m\n",
            "[2026-02-14 11:57:40]   train: batch 545/898  loss=0.9690  0.15s/batch  ETA=0.9m\n",
            "[2026-02-14 11:57:50]   train: batch 614/898  loss=0.6969  0.15s/batch  ETA=0.7m\n",
            "[2026-02-14 11:58:00]   train: batch 686/898  loss=0.5563  0.15s/batch  ETA=0.5m\n",
            "[2026-02-14 11:58:10]   train: batch 754/898  loss=0.9153  0.15s/batch  ETA=0.4m\n",
            "[2026-02-14 11:58:20]   train: batch 821/898  loss=0.7593  0.15s/batch  ETA=0.2m\n",
            "[2026-02-14 11:58:30]   train: batch 888/898  loss=0.9189  0.15s/batch  ETA=0.0m\n",
            "[2026-02-14 11:58:31] train Loss: 0.8539 Acc: 0.9674\n",
            "[2026-02-14 11:58:41]   test: batch 121/225  loss=1.6824  0.08s/batch  ETA=0.1m\n",
            "[2026-02-14 11:58:50] test Loss: 1.6981 Acc: 0.6690\n",
            "[2026-02-14 11:58:50] Epoch time: 2.51 min\n",
            "[2026-02-14 11:58:50] Epoch 29/30\n",
            "[2026-02-14 11:58:50] LR: 0.000004\n",
            "[2026-02-14 11:59:00]   train: batch 72/898  loss=0.7251  0.14s/batch  ETA=1.9m\n",
            "[2026-02-14 11:59:10]   train: batch 140/898  loss=0.6970  0.14s/batch  ETA=1.8m\n",
            "[2026-02-14 11:59:20]   train: batch 206/898  loss=0.9227  0.15s/batch  ETA=1.7m\n",
            "[2026-02-14 11:59:30]   train: batch 273/898  loss=0.5097  0.15s/batch  ETA=1.5m\n",
            "[2026-02-14 11:59:40]   train: batch 340/898  loss=0.6756  0.15s/batch  ETA=1.4m\n",
            "[2026-02-14 11:59:50]   train: batch 408/898  loss=0.8372  0.15s/batch  ETA=1.2m\n",
            "[2026-02-14 12:00:01]   train: batch 479/898  loss=0.7662  0.15s/batch  ETA=1.0m\n",
            "[2026-02-14 12:00:11]   train: batch 547/898  loss=0.8600  0.15s/batch  ETA=0.9m\n",
            "[2026-02-14 12:00:21]   train: batch 613/898  loss=0.8812  0.15s/batch  ETA=0.7m\n"
          ]
        }
      ],
      "source": [
        "# Run training script\n",
        "# Note: Make sure dataset is in train/ and test/ folders\n",
        "!python src/train_model.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wv6RDdsg-98L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSM9nkwNwlst"
      },
      "source": [
        "## 7. Streamlit Demo (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8IJl3wewlst"
      },
      "outputs": [],
      "source": [
        "# Run Streamlit app (requires ngrok or localtunnel for Colab)\n",
        "# !pip install streamlit ngrok\n",
        "# !ngrok authtoken YOUR_TOKEN  # Add your ngrok token\n",
        "# !streamlit run src/streamlit_app.py &"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}