import os
import cv2
import torch
import time
import numpy as np
from torchvision import transforms, models
import torch.nn as nn

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
model_path = os.environ.get('MODEL_PATH', 'wafflelover404_emotion_model.pth')
img_size = int(os.environ.get('IMG_SIZE', '128'))
classes = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']
num_classes = len(classes)

# –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
if torch.cuda.is_available():
    device = torch.device("cuda:0")
elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
    device = torch.device("mps")
else:
    device = torch.device("cpu")

def load_model(model_path: str, num_classes: int) -> torch.nn.Module:
    # Load the state dict first to detect architecture
    state = torch.load(model_path, map_location=device)
    
    # Detect model architecture from state dict keys
    if 'conv1.weight' in state:
        # ResNet architecture
        model = models.resnet18()
        model.fc = nn.Linear(model.fc.in_features, num_classes)
    elif 'features.0.0.weight' in state:
        # EfficientNet or MobileNet architecture - check filename
        if 'efficientnet' in model_path.lower():
            from torchvision.models import efficientnet_b0
            model = efficientnet_b0()
            model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)
        elif 'mobilenet' in model_path.lower():
            from torchvision.models import mobilenet_v2
            model = mobilenet_v2()
            model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)
        else:
            # Default to EfficientNet if unsure
            from torchvision.models import efficientnet_b0
            model = efficientnet_b0()
            model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)
    else:
        # Fallback to ResNet
        model = models.resnet18()
        model.fc = nn.Linear(model.fc.in_features, num_classes)
    
    model.load_state_dict(state)
    model = model.to(device)
    model.eval()
    return model

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
model = load_model(model_path, num_classes)

# –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize(img_size + 32),
    transforms.CenterCrop(img_size),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# –î–µ—Ç–µ–∫—Ü–∏—è –ª–∏—Ü
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

def benchmark_inference():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞"""
    print("üöÄ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞...")
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (48x48 –∫–∞–∫ –≤ FER2013)
    test_image = np.random.randint(0, 255, (48, 48, 3), dtype=np.uint8)
    
    # –ü—Ä–æ–≥—Ä–µ–≤ –º–æ–¥–µ–ª–∏
    for _ in range(5):
        input_tensor = transform(test_image).unsqueeze(0).to(device)
        with torch.no_grad():
            _ = model(input_tensor)
    
    # –ò–∑–º–µ—Ä–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏
    times = []
    num_iterations = 100
    
    for i in range(num_iterations):
        start_time = time.time()
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        input_tensor = transform(test_image).unsqueeze(0).to(device)
        
        # –ò–Ω—Ñ–µ—Ä–µ–Ω—Å
        with torch.no_grad():
            outputs = model(input_tensor)
            probs = torch.softmax(outputs, dim=1)
            confidence, pred = torch.max(probs, 1)
        
        end_time = time.time()
        iteration_time = end_time - start_time
        times.append(iteration_time)
        
        if i < 5:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            emotion = classes[pred.item()]
            conf = confidence.item()
            print(f"  –ò—Ç–µ—Ä–∞—Ü–∏—è {i+1}: {emotion} (confidence: {conf:.3f}) - {iteration_time:.4f}s")
    
    avg_time = np.mean(times)
    min_time = np.min(times)
    max_time = np.max(times)
    fps = 1.0 / avg_time
    
    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ({num_iterations} –∏—Ç–µ—Ä–∞—Ü–∏–π):")
    print(f"  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {avg_time:.4f}s")
    print(f"  –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ: {min_time:.4f}s")
    print(f"  –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ: {max_time:.4f}s")
    print(f"  –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π FPS: {fps:.1f}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è ‚â§2—Å
    if avg_time <= 2.0:
        print(f"‚úÖ –¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ: —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è {avg_time:.4f}s ‚â§ 2s")
    else:
        print(f"‚ùå –¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ: —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è {avg_time:.4f}s > 2s")
    
    return avg_time <= 2.0

def benchmark_full_pipeline():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω–≤–µ–π–µ—Ä–∞ —Å –¥–µ—Ç–µ–∫—Ü–∏–µ–π –ª–∏—Ü"""
    print("\nüéØ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω–≤–µ–π–µ—Ä–∞ (–¥–µ—Ç–µ–∫—Ü–∏—è + –∏–Ω—Ñ–µ—Ä–µ–Ω—Å)...")
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –∫–∞–¥—Ä –±–æ–ª—å—à–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
    test_frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
    
    times = []
    num_iterations = 50
    
    for i in range(num_iterations):
        start_time = time.time()
        
        # –î–µ—Ç–µ–∫—Ü–∏—è –ª–∏—Ü
        gray = cv2.cvtColor(test_frame, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))
        
        # –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ª–∏—Ü–∞
        for (x, y, w, h) in faces:
            face_roi = test_frame[y:y+h, x:x+w]
            if face_roi.size > 0:
                input_tensor = transform(face_roi).unsqueeze(0).to(device)
                with torch.no_grad():
                    outputs = model(input_tensor)
                    probs = torch.softmax(outputs, dim=1)
                    confidence, pred = torch.max(probs, 1)
        
        end_time = time.time()
        iteration_time = end_time - start_time
        times.append(iteration_time)
    
    avg_time = np.mean(times)
    fps = 1.0 / avg_time
    
    print(f"  –ü–æ–ª–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä - —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {avg_time:.4f}s")
    print(f"  –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π FPS —Å –¥–µ—Ç–µ–∫—Ü–∏–µ–π: {fps:.1f}")
    
    return avg_time <= 2.0

if __name__ == "__main__":
    print("=== –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ===")
    print(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    print(f"–†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {img_size}x{img_size}")
    print(f"–ú–æ–¥–µ–ª—å: {model_path}")
    print()
    
    # –¢–µ—Å—Ç –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
    inference_ok = benchmark_inference()
    
    # –¢–µ—Å—Ç –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω–≤–µ–π–µ—Ä–∞
    pipeline_ok = benchmark_full_pipeline()
    
    print(f"\nüéØ –ò—Ç–æ–≥–∏:")
    print(f"  –ò–Ω—Ñ–µ—Ä–µ–Ω—Å: {'‚úÖ' if inference_ok else '‚ùå'}")
    print(f"  –ü–æ–ª–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä: {'‚úÖ' if pipeline_ok else '‚ùå'}")
    
    if inference_ok and pipeline_ok:
        print("üöÄ –í—Å–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã!")
    else:
        print("‚ö†Ô∏è –¢—Ä–µ–±—É—é—Ç—Å—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º")
