import os
import json
import zipfile
import tempfile
import time
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple

import cv2
import numpy as np
import pandas as pd
import seaborn as sns
import streamlit as st
import torch
import torch.nn as nn
from PIL import Image
from sklearn.metrics import auc, classification_report, confusion_matrix, f1_score, roc_curve
from sklearn.preprocessing import label_binarize
from torchvision import datasets, models, transforms
import matplotlib.pyplot as plt

CLASSES = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']

def _get_device() -> torch.device:
    if torch.cuda.is_available():
        return torch.device("cuda:0")
    if hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        return torch.device("mps")
    return torch.device("cpu")

@st.cache_resource
def load_model(model_path: str, num_classes: int) -> Tuple[torch.nn.Module, torch.device]:
    device = _get_device()

    state = torch.load(model_path, map_location=device)

    if 'conv1.weight' in state:
        model = models.resnet18()
        model.fc = nn.Linear(model.fc.in_features, num_classes)
    elif 'features.0.0.weight' in state:
        if 'efficientnet' in model_path.lower():
            from torchvision.models import efficientnet_b0
            model = efficientnet_b0()
            model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)
        elif 'mobilenet' in model_path.lower():
            from torchvision.models import mobilenet_v2
            model = mobilenet_v2()
            model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)
        else:
            from torchvision.models import efficientnet_b0
            model = efficientnet_b0()
            model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)
    else:
        model = models.resnet18()
        model.fc = nn.Linear(model.fc.in_features, num_classes)

    model.load_state_dict(state)
    model = model.to(device)
    model.eval()
    return model, device

@st.cache_resource
def get_face_cascade() -> cv2.CascadeClassifier:
    return cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

def get_transform(img_size: int) -> transforms.Compose:
    return transforms.Compose(
        [
            transforms.ToPILImage(),
            transforms.Resize(img_size + 32),
            transforms.CenterCrop(img_size),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ]
    )

def rgb_to_bgr(image_rgb: np.ndarray) -> np.ndarray:
    return cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)

def predict_on_bgr_image(
    image_bgr: np.ndarray,
    model: torch.nn.Module,
    device: torch.device,
    img_size: int,
) -> np.ndarray:
    tfm = get_transform(img_size)
    x = tfm(image_bgr).unsqueeze(0).to(device)
    with torch.no_grad():
        logits = model(x)
        probs = torch.softmax(logits, dim=1).squeeze(0).detach().cpu().numpy()
    return probs

def detect_faces(image_bgr: np.ndarray, scale_factor: float, min_neighbors: int) -> List[Tuple[int, int, int, int]]:
    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)
    faces = get_face_cascade().detectMultiScale(
        gray, scaleFactor=scale_factor, minNeighbors=min_neighbors, minSize=(30, 30)
    )
    return [(int(x), int(y), int(w), int(h)) for (x, y, w, h) in faces]

def overlay_prediction(
    frame_bgr: np.ndarray,
    faces: List[Tuple[int, int, int, int]],
    probs_list: List[np.ndarray],
) -> np.ndarray:
    out = frame_bgr.copy()
    for (x, y, w, h), probs in zip(faces, probs_list):
        pred_idx = int(np.argmax(probs))
        label = CLASSES[pred_idx]
        conf = float(probs[pred_idx])
        cv2.rectangle(out, (x, y), (x + w, y + h), (255, 0, 0), 2)
        cv2.putText(
            out,
            f'{label}: {conf:.2f}',
            (x, max(0, y - 10)),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.9,
            (255, 0, 0),
            2,
        )
    return out

def probs_to_table(probs: np.ndarray) -> Dict[str, float]:
    return {CLASSES[i]: float(probs[i]) for i in range(len(CLASSES))}

def get_available_models():
    models_dir = "models"
    if not os.path.exists(models_dir):
        return []

    model_files = []
    for file in os.listdir(models_dir):
        if file.endswith('.pth'):
            model_files.append(os.path.join(models_dir, file))
    return model_files

def get_available_test_sets():
    test_sets = {}

    if os.path.exists("test") and os.path.isdir("test"):
        test_sets["–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π test"] = "test"

    for item in os.listdir("."):
        if os.path.isdir(item) and item != "test" and not item.startswith('.'):
            has_emotion_dirs = False
            for emotion in CLASSES:
                if os.path.exists(os.path.join(item, emotion)):
                    has_emotion_dirs = True
                    break
            if has_emotion_dirs:
                test_sets[item] = item

    return test_sets

def run_model_tests(model, model_path, test_path, device, img_size, batch_size):
    start_time = pd.Timestamp.now()

    with st.spinner(f'üîÑ –¢–µ—Å—Ç–∏—Ä—É—é –º–æ–¥–µ–ª—å {os.path.basename(model_path)}...'):
        progress_bar = st.progress(0)
        status_text = st.empty()

        status_text.text('üìÇ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...')
        progress_bar.progress(0.2)

        data_transform = transforms.Compose([
            transforms.Resize(img_size + 32),
            transforms.CenterCrop(img_size),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])

        ds = datasets.ImageFolder(test_path, data_transform)
        loader = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0)

        total_images = len(ds)
        expected_batches = (total_images + batch_size - 1) // batch_size
        status_text.text(f'üìä –ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {total_images} (–æ–∂–∏–¥–∞–µ—Ç—Å—è –±–∞—Ç—á–µ–π: {expected_batches})')
        progress_bar.progress(0.4)

        status_text.text('üß† –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...')
        progress_bar.progress(0.6)

        all_labels: List[int] = []
        all_preds: List[int] = []
        all_probs: List[np.ndarray] = []

        model.eval()
        batch_count = 0

        with torch.no_grad():
            for inputs, labels in loader:
                inputs = inputs.to(device)
                labels = labels.to(device)
                logits = model(inputs)
                probs = torch.softmax(logits, dim=1)
                preds = torch.argmax(probs, dim=1)

                all_labels.extend(labels.detach().cpu().numpy().tolist())
                all_preds.extend(preds.detach().cpu().numpy().tolist())
                all_probs.extend(probs.detach().cpu().numpy())

                batch_count += 1
                progress = 0.6 + (batch_count / len(loader)) * 0.3
                progress_bar.progress(progress)
                status_text.text(f'üß† –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {batch_count}/{len(loader)} –±–∞—Ç—á–µ–π...')

        status_text.text('üìà –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫...')
        progress_bar.progress(0.9)

        y_true = np.array(all_labels)
        y_pred = np.array(all_preds)
        y_prob = np.array(all_probs)

        acc = float(np.mean(y_true == y_pred))
        f1m = float(f1_score(y_true, y_pred, average='macro'))
        cm = confusion_matrix(y_true, y_pred)
        rep = classification_report(y_true, y_pred, target_names=CLASSES)

        y_true_bin = label_binarize(y_true, classes=list(range(len(CLASSES))))
        roc_auc: Dict[str, float] = {}

        fig = plt.figure(figsize=(10, 8))
        for i, cls in enumerate(CLASSES):
            fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_prob[:, i])
            roc_auc[cls] = float(auc(fpr, tpr))
            plt.plot(fpr, tpr, label=f'{cls} (AUC = {roc_auc[cls]:.2f})')
        plt.plot([0, 1], [0, 1], 'k--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curve')
        plt.legend(loc='lower right')

        progress_bar.progress(1.0)

        end_time = pd.Timestamp.now()
        duration = (end_time - start_time).total_seconds()

        status_text.text(f'‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {duration:.2f} —Å–µ–∫')

        test_info = {
            'model_path': model_path,
            'test_path': test_path,
            'img_size': img_size,
            'batch_size': batch_size,
            'total_images': total_images,
            'total_batches': len(loader),
            'duration_seconds': duration,
            'images_per_second': total_images / duration if duration > 0 else 0,
            'start_time': start_time.isoformat(),
            'end_time': end_time.isoformat(),
            'device': str(device)
        }

    return TestMetrics(
        accuracy=acc,
        f1_macro=f1m,
        confusion=cm,
        report=rep,
        roc_auc=roc_auc,
        fig_roc=fig,
    ), test_info

def display_model_results(model, model_path, test_path, device, img_size, batch_size, model_name=None):
    try:
        with st.spinner(f'üîÑ –¢–µ—Å—Ç–∏—Ä—É—é –º–æ–¥–µ–ª—å {model_name or os.path.basename(model_path)}...'):
            metrics, test_info = run_model_tests(
                model=model,
                model_path=model_path,
                test_path=test_path,
                device=device,
                img_size=img_size,
                batch_size=batch_size
            )

        st.success(f"‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ {model_name or os.path.basename(model_path)} –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

        with st.expander("üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–µ—Å—Ç–µ", expanded=True):
            col1, col2, col3, col4 = st.columns(4)

            with col1:
                st.metric("üìÅ –§–∞–π–ª –º–æ–¥–µ–ª–∏", os.path.basename(test_info['model_path']))
                st.metric("üìÇ –¢–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä", os.path.basename(test_info['test_path']))

            with col2:
                st.metric("üñºÔ∏è –í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", test_info['total_images'])
                st.metric("üì¶ –ë–∞—Ç—á–µ–π", test_info['total_batches'])

            with col3:
                st.metric("‚è±Ô∏è –í—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è", f"{test_info['duration_seconds']:.2f} —Å–µ–∫")
                st.metric("üöÄ –°–∫–æ—Ä–æ—Å—Ç—å", f"{test_info['images_per_second']:.1f} img/—Å–µ–∫")

            with col4:
                st.metric("üìè –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", f"{test_info['img_size']}x{test_info['img_size']}")
                st.metric("üíæ Batch size", test_info['batch_size'])

        col1, col2, col3 = st.columns(3)
        col1.metric("Accuracy", f'{metrics.accuracy:.4f}')
        col2.metric("F1 Macro", f'{metrics.f1_macro:.4f}')
        col3.metric("Avg ROC AUC", f'{np.mean(list(metrics.roc_auc.values())):.4f}')

        st.markdown("### üìä ROC AUC –ø–æ –∫–ª–∞—Å—Å–∞–º")
        roc_df = pd.DataFrame([
            {'–ö–ª–∞—Å—Å': cls, 'ROC AUC': f'{auc:.4f}'}
            for cls, auc in metrics.roc_auc.items()
        ])
        st.dataframe(roc_df, use_container_width=True)

        st.markdown("### üéØ Confusion Matrix")
        fig, ax = plt.subplots(figsize=(8, 6))
        sns.heatmap(metrics.confusion, annot=True, fmt='d', cmap='Blues',
                   xticklabels=CLASSES, yticklabels=CLASSES, ax=ax)
        ax.set_title('Confusion Matrix')
        ax.set_xlabel('Predicted')
        ax.set_ylabel('Actual')
        st.pyplot(fig)

        st.markdown("### üìã Classification Report")
        st.text(metrics.report)

        return metrics, test_info

    except Exception as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")
        return None, None

def get_rank_color(rank):
    if rank == 1:
        return "ü•á"
    elif rank == 2:
        return "ü•à"
    elif rank == 3:
        return "ü•â"
    else:
        return f"#{rank}"

def test_all_models(available_models, test_path, device, img_size, batch_size):
    if not available_models:
        st.warning("‚ö†Ô∏è –ú–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –ø–∞–ø–∫–µ models/")
        return []

    all_results = []
    all_test_infos = []
    progress_bar = st.progress(0)
    status_text = st.empty()

    for i, model_file in enumerate(available_models):
        try:
            progress = (i + 1) / len(available_models)
            progress_bar.progress(progress)
            status_text.text(f'üîÑ –¢–µ—Å—Ç–∏—Ä—É—é {i+1}/{len(available_models)}: {os.path.basename(model_file)}')

            test_model = models.resnet18()
            test_model.fc = nn.Linear(test_model.fc.in_features, len(CLASSES))
            state = torch.load(model_file, map_location=device)
            test_model.load_state_dict(state)
            test_model = test_model.to(device)
            test_model.eval()

            metrics, test_info = run_model_tests(
                model=test_model,
                model_path=model_file,
                test_path=test_path,
                device=device,
                img_size=img_size,
                batch_size=batch_size
            )

            all_results.append({
                'model': os.path.basename(model_file),
                'model_path': model_file,
                'accuracy': metrics.accuracy,
                'f1_macro': metrics.f1_macro,
                'roc_auc_avg': np.mean(list(metrics.roc_auc.values())),
                'roc_auc': metrics.roc_auc,
                'confusion': metrics.confusion,
                'report': metrics.report,
                'fig_roc': metrics.fig_roc,
                'error': None,
                'test_info': test_info
            })

            all_test_infos.append(test_info)

        except Exception as e:
            all_results.append({
                'model': os.path.basename(model_file),
                'model_path': model_file,
                'accuracy': 0.0,
                'f1_macro': 0.0,
                'roc_auc_avg': 0.0,
                'roc_auc': {},
                'confusion': None,
                'report': '',
                'fig_roc': None,
                'error': str(e),
                'test_info': None
            })

    progress_bar.progress(1.0)
    status_text.text('‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã!')

    return all_results, all_test_infos

def create_comparison_visualization(all_results):
    if not all_results:
        return None, None

    df = pd.DataFrame([
        {
            'Model': result['model'],
            'Accuracy': result['accuracy'],
            'F1 Macro': result['f1_macro'],
            'Avg ROC AUC': result['roc_auc_avg']
        }
        for result in all_results
        if 'error' not in result
    ])

    df = df.sort_values('Accuracy', ascending=False)
    df['Rank'] = range(1, len(df) + 1)

    winner = df.iloc[0]

    colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(df)))
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))

    bars1 = ax1.bar(range(len(df)), df['Accuracy'], width=0.6, color=colors)
    ax1.set_title('üèÜ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ Accuracy', fontweight='bold')
    ax1.set_xlabel('–ú–æ–¥–µ–ª–∏')
    ax1.set_ylabel('Accuracy')
    ax1.set_xticks(range(len(df)))
    ax1.set_xticklabels(df['Model'], rotation=45, ha='right')
    ax1.grid(True, alpha=0.3)

    bars2 = ax2.bar(range(len(df)), df['F1 Macro'], width=0.6, color=colors)
    ax2.set_title('ü•à –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ F1 Macro', fontweight='bold')
    ax2.set_xlabel('–ú–æ–¥–µ–ª–∏')
    ax2.set_ylabel('F1 Macro')
    ax2.set_xticks(range(len(df)))
    ax2.set_xticklabels(df['Model'], rotation=45, ha='right')
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    return fig, df, winner

def export_results(df, winner):

    export_df = df.copy()
    export_df.columns = ['Rank', 'Model', 'Accuracy', 'F1_Macro', 'Avg_ROC_AUC']

    csv_ranked = export_df.to_csv(index=False)

    stats_df = df[['Model', 'Accuracy', 'F1_Macro', 'Avg_ROC_AUC']].copy()
    stats_df.columns = ['Model', 'Accuracy', 'F1_Macro', 'Avg_ROC_AUC']

    csv_stats = stats_df.to_csv(index=False)

    return csv_ranked, csv_stats

def process_zip_file(zip_file, model, device, img_size, detect_faces_flag, scale_factor, min_neighbors):

    with tempfile.TemporaryDirectory() as temp_dir:

        with zipfile.ZipFile(zip_file, 'r') as zip_ref:
            zip_ref.extractall(temp_dir)

        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}
        image_files = []

        for root, dirs, files in os.walk(temp_dir):
            for file in files:
                if os.path.splitext(file.lower())[1] in image_extensions:
                    image_files.append(os.path.join(root, file))

        if not image_files:
            return None, "–í –∞—Ä—Ö–∏–≤–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"

        results = []
        total_faces = 0

        progress_bar = st.progress(0)
        status_text = st.empty()

        for i, img_path in enumerate(image_files):
            try:

                progress = (i + 1) / len(image_files)
                progress_bar.progress(progress)
                status_text.text(f'–û–±—Ä–∞–±–æ—Ç–∫–∞ {i+1}/{len(image_files)}: {os.path.basename(img_path)}')

                img = Image.open(img_path).convert('RGB')
                img_rgb = np.array(img)
                img_bgr = rgb_to_bgr(img_rgb)

                if detect_faces_flag:
                    faces = detect_faces(img_bgr, scale_factor=scale_factor, min_neighbors=min_neighbors)

                    if len(faces) > 0:
                        for (x, y, w, h) in faces:
                            roi = img_bgr[y:y+h, x:x+w]
                            if roi.size > 0:
                                probs = predict_on_bgr_image(roi, model=model, device=device, img_size=img_size)
                                pred_idx = int(np.argmax(probs))
                                label = CLASSES[pred_idx]
                                conf = float(probs[pred_idx])

                                results.append({
                                    'file': os.path.basename(img_path),
                                    'emotion': label,
                                    'confidence': conf,
                                    'face_count': len(faces)
                                })
                                total_faces += 1
                    else:

                        probs = predict_on_bgr_image(img_bgr, model=model, device=device, img_size=img_size)
                        pred_idx = int(np.argmax(probs))
                        label = CLASSES[pred_idx]
                        conf = float(probs[pred_idx])

                        results.append({
                            'file': os.path.basename(img_path),
                            'emotion': label,
                            'confidence': conf,
                            'face_count': 0
                        })
                else:

                    probs = predict_on_bgr_image(img_bgr, model=model, device=device, img_size=img_size)
                    pred_idx = int(np.argmax(probs))
                    label = CLASSES[pred_idx]
                    conf = float(probs[pred_idx])

                    results.append({
                        'file': os.path.basename(img_path),
                        'emotion': label,
                        'confidence': conf,
                        'face_count': 1
                    })
                    total_faces += 1

            except Exception as e:
                results.append({
                    'file': os.path.basename(img_path),
                    'emotion': 'error',
                    'confidence': 0.0,
                    'face_count': 0,
                    'error': str(e)
                })

        progress_bar.progress(1.0)
        status_text.text('–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!')

        return results, None

@dataclass
class TestMetrics:
    accuracy: float
    f1_macro: float
    confusion: np.ndarray
    report: str
    roc_auc: Dict[str, float]
    fig_roc: "object"

def compute_test_metrics(
    model: torch.nn.Module,
    device: torch.device,
    test_path: str,
    img_size: int,
    batch_size: int,
) -> TestMetrics:
    import matplotlib.pyplot as plt

    data_transform = transforms.Compose(
        [
            transforms.Resize(img_size + 32),
            transforms.CenterCrop(img_size),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ]
    )

    ds = datasets.ImageFolder(test_path, data_transform)
    loader = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0)

    all_labels: List[int] = []
    all_preds: List[int] = []
    all_probs: List[np.ndarray] = []

    model.eval()
    with torch.no_grad():
        for inputs, labels in loader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            logits = model(inputs)
            probs = torch.softmax(logits, dim=1)
            preds = torch.argmax(probs, dim=1)
            all_labels.extend(labels.detach().cpu().numpy().tolist())
            all_preds.extend(preds.detach().cpu().numpy().tolist())
            all_probs.extend(probs.detach().cpu().numpy())

    y_true = np.array(all_labels)
    y_pred = np.array(all_preds)
    y_prob = np.array(all_probs)

    acc = float(np.mean(y_true == y_pred))
    f1m = float(f1_score(y_true, y_pred, average='macro'))
    cm = confusion_matrix(y_true, y_pred)
    rep = classification_report(y_true, y_pred, target_names=CLASSES)

    y_true_bin = label_binarize(y_true, classes=list(range(len(CLASSES))))
    roc_auc: Dict[str, float] = {}

    fig = plt.figure()
    for i, cls in enumerate(CLASSES):
        fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_prob[:, i])
        roc_auc[cls] = float(auc(fpr, tpr))
        plt.plot(fpr, tpr, label=f'{cls} (AUC = {roc_auc[cls]:.2f})')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend(loc='lower right')

    return TestMetrics(
        accuracy=acc,
        f1_macro=f1m,
        confusion=cm,
        report=rep,
        roc_auc=roc_auc,
        fig_roc=fig,
    )

def read_saved_metrics(metrics_dir: str) -> Dict[str, object]:
    out: Dict[str, object] = {}

    acc_path = os.path.join(metrics_dir, 'accuracy.txt')
    f1_path = os.path.join(metrics_dir, 'f1_score.txt')
    cm_path = os.path.join(metrics_dir, 'confusion_matrix.txt')
    roc_json = os.path.join(metrics_dir, 'roc_auc.json')
    rep_path = os.path.join(metrics_dir, 'classification_report.txt')
    roc_img = os.path.join(metrics_dir, 'roc_curve.png')

    if os.path.isfile(acc_path):
        with open(acc_path, 'r', encoding='utf-8') as f:
            out['accuracy'] = float(f.read().strip())
    if os.path.isfile(f1_path):
        with open(f1_path, 'r', encoding='utf-8') as f:
            out['f1_score'] = float(f.read().strip())
    if os.path.isfile(cm_path):
        try:
            out['confusion_matrix'] = np.loadtxt(cm_path, dtype=int)
        except Exception:
            pass
    if os.path.isfile(roc_json):
        with open(roc_json, 'r', encoding='utf-8') as f:
            raw = json.load(f)
        out['roc_auc'] = {CLASSES[int(k)]: float(v) for k, v in raw.items()} if raw else {}
    if os.path.isfile(rep_path):
        with open(rep_path, 'r', encoding='utf-8') as f:
            out['classification_report'] = f.read()
    if os.path.isfile(roc_img):
        out['roc_curve_png'] = roc_img

    return out

def main() -> None:
    st.set_page_config(page_title='–ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–π', layout='wide')

    st.title('–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —ç–º–æ—Ü–∏–π –ø–æ –ª–∏—Ü—É')

    available_models = get_available_models()
    available_test_sets = get_available_test_sets()

    with st.sidebar:
        st.header('–ù–∞—Å—Ç—Ä–æ–π–∫–∏')

        if available_models:
            model_options = [os.path.basename(m) for m in available_models]
            selected_model_name = st.selectbox('–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å:', model_options)
            model_path = available_models[model_options.index(selected_model_name)]
        else:
            st.warning("–ú–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –ø–∞–ø–∫–µ models/")
            model_path = os.environ.get('MODEL_PATH', 'models/wafflelover404_emotion_model.pth')

        if available_test_sets:
            selected_test_set_name = st.selectbox('–í—ã–±–µ—Ä–∏—Ç–µ —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä:', list(available_test_sets.keys()))
            test_path = available_test_sets[selected_test_set_name]
        else:
            st.warning("–¢–µ—Å—Ç–æ–≤—ã–µ –Ω–∞–±–æ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            test_path = "test"

        st.write("**–ò–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Å–≤–æ–π —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä:**")
        uploaded_test = st.file_uploader('ZIP —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏', type=['zip'], key='test_upload')

        if uploaded_test is not None:
            with tempfile.TemporaryDirectory() as temp_dir:
                with zipfile.ZipFile(uploaded_test, 'r') as zip_ref:
                    zip_ref.extractall(temp_dir)
                test_path = temp_dir
                st.success(f"–ó–∞–≥—Ä—É–∂–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä: {temp_dir}")

        img_size = int(os.environ.get('IMG_SIZE', '128'))
        detect_faces_flag = st.toggle('–ò—Å–∫–∞—Ç—å –ª–∏—Ü–∞', value=True)
        show_probs = st.toggle('–ü–æ–∫–∞–∑—ã–≤–∞—Ç—å —Ç–∞–±–ª–∏—Ü—É –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π', value=True)
        scale_factor = st.slider('–ú–∞—Å—à—Ç–∞–± (–¥–µ—Ç–µ–∫—Ü–∏—è –ª–∏—Ü)', min_value=1.05, max_value=1.50, value=1.10, step=0.01)
        min_neighbors = st.slider('–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Å–æ—Å–µ–¥–∏', min_value=3, max_value=10, value=5, step=1)
        batch_size = st.number_input('Batch size', min_value=1, max_value=128, value=32)

    if not os.path.isfile(model_path):
        st.error('–§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω. –í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å –≤ –ø–∞–ø–∫—É models/.')
        st.stop()

    model, device = load_model(model_path=model_path, num_classes=len(CLASSES))

    tab_single, tab_batch, tab_metrics, tab_testing, tab_camera = st.tabs(['–û–¥–∏–Ω–æ—á–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ', '–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ ZIP', '–ú–µ—Ç—Ä–∏–∫–∏ –∏ ROC', '–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏', '–ö–∞–º–µ—Ä–∞'])

    with tab_single:
        st.subheader('–ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è')

        uploaded_file = st.file_uploader('–í—ã–±–µ—Ä–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (jpg/png)', type=['jpg', 'jpeg', 'png'])

        if uploaded_file is not None:
            try:
                image = Image.open(uploaded_file).convert('RGB')
                st.success("–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ")

                img_rgb = np.array(image)
                img_bgr = rgb_to_bgr(img_rgb)

                col_img, col_out = st.columns([2, 1])

                with col_img:
                    st.image(img_rgb, caption='–ò—Å—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ', use_container_width=True)

                if detect_faces_flag:
                    faces = detect_faces(img_bgr, scale_factor=scale_factor, min_neighbors=min_neighbors)
                    probs_list: List[np.ndarray] = []

                    if len(faces) > 0:
                        for (x, y, w, h) in faces:
                            roi = img_bgr[y : y + h, x : x + w]
                            if roi.size > 0:
                                probs = predict_on_bgr_image(roi, model=model, device=device, img_size=img_size)
                                probs_list.append(probs)

                        if probs_list:
                            overlay = overlay_prediction(img_bgr, faces, probs_list)
                            with col_img:
                                st.image(overlay, caption='–†–µ–∑—É–ª—å—Ç–∞—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏', use_container_width=True, channels='RGB')

                            if show_probs:
                                with col_out:
                                    st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
                                    for i, ((x, y, w, h), probs) in enumerate(zip(faces, probs_list)):
                                        pred_idx = int(np.argmax(probs))
                                        label = CLASSES[pred_idx]
                                        conf = float(probs[pred_idx])

                                        st.write(f"**–õ–∏—Ü–æ {i+1}:** {label}")
                                        st.write(f"**–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:** {conf:.3f}")
                                        st.write("**–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏:**")
                                        st.table(probs_to_table(probs))
                                        st.divider()
                        else:
                            with col_out:
                                st.warning("–õ–∏—Ü–∞ –Ω–∞–π–¥–µ–Ω—ã, –Ω–æ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å ROI")
                    else:
                        with col_out:
                            st.info("–õ–∏—Ü–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

                            st.write("–ê–Ω–∞–ª–∏–∑ –≤—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:")
                            probs = predict_on_bgr_image(img_bgr, model=model, device=device, img_size=img_size)
                            pred_idx = int(np.argmax(probs))
                            label = CLASSES[pred_idx]
                            conf = float(probs[pred_idx])

                            st.write(f"**–≠–º–æ—Ü–∏—è:** {label}")
                            st.write(f"**–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:** {conf:.3f}")
                            if show_probs:
                                st.write("**–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏:**")
                                st.table(probs_to_table(probs))
                else:

                    with col_out:
                        st.write("–ê–Ω–∞–ª–∏–∑ –≤—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:")
                        probs = predict_on_bgr_image(img_bgr, model=model, device=device, img_size=img_size)
                        pred_idx = int(np.argmax(probs))
                        label = CLASSES[pred_idx]
                        conf = float(probs[pred_idx])

                        st.write(f"**–≠–º–æ—Ü–∏—è:** {label}")
                        st.write(f"**–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:** {conf:.3f}")
                        if show_probs:
                            st.write("**–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏:**")
                            st.table(probs_to_table(probs))
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
        else:
            st.info("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")

    with tab_batch:
        st.subheader('–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ ZIP –∞—Ä—Ö–∏–≤–∞')

        zip_file = st.file_uploader('–í—ã–±–µ—Ä–∏—Ç–µ ZIP –∞—Ä—Ö–∏–≤ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏', type=['zip'])

        if zip_file is not None:
            st.info("–ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É ZIP –∞—Ä—Ö–∏–≤–∞...")

            results, error = process_zip_file(
                zip_file, model, device, img_size,
                detect_faces_flag, scale_factor, min_neighbors
            )

            if error:
                st.error(error)
            elif results:
                st.success(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(results)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π!")

                df_data = []
                for result in results:
                    if result['emotion'] != 'error':
                        df_data.append(result)

                if df_data:
                    df = pd.DataFrame(df_data)

                    st.subheader("–û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
                    col1, col2, col3, col4 = st.columns(4)

                    with col1:
                        st.metric("–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", len(df))
                    with col2:
                        st.metric("–ù–∞–π–¥–µ–Ω–æ –ª–∏—Ü", df['face_count'].sum())
                    with col3:
                        avg_conf = df[df['emotion'] != 'error']['confidence'].mean()
                        st.metric("–°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å", f"{avg_conf:.3f}")
                    with col4:
                        error_count = len(results) - len(df)
                        st.metric("–û—à–∏–±–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏", error_count)

                    st.subheader("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —ç–º–æ—Ü–∏–π")
                    emotion_counts = df['emotion'].value_counts()
                    st.bar_chart(emotion_counts)

                    st.subheader("–î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
                    st.dataframe(df, use_container_width=True)

                    st.subheader("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏")
                    fig, ax = plt.subplots()
                    ax.hist(df['confidence'], bins=20, alpha=0.7)
                    ax.set_xlabel('–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å')
                    ax.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')
                    ax.set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π')
                    st.pyplot(fig)

                    csv = df.to_csv(index=False)
                    st.download_button(
                        label="–°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (CSV)",
                        data=csv,
                        file_name="emotion_analysis_results.csv",
                        mime="text/csv"
                    )
                else:
                    st.warning("–ù–µ—Ç —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")

    with tab_metrics:
        st.subheader('–ú–µ—Ç—Ä–∏–∫–∏ –∏ ROC –º–æ–¥–µ–ª–∏')

        col_test1, col_test2 = st.columns([2, 1])
        with col_test1:
            test_path = st.text_input('–ü—É—Ç—å –∫ —Ç–µ—Å—Ç–æ–≤–æ–º—É –Ω–∞–±–æ—Ä—É', value='test', key='metrics_test_path')
        with col_test2:
            st.metric("Batch size", batch_size)

        calculate_button = st.button('üìä –í—ã—á–∏—Å–ª–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏', type='primary')

        if calculate_button:
            if not os.path.isdir(test_path):
                st.error('–ü–∞–ø–∫–∞ test –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –£–∫–∞–∂–∏—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –ø—É—Ç—å.')
            else:
                start_time = pd.Timestamp.now()

                with st.spinner('üîÑ –í—ã—á–∏—Å–ª—è—é –º–µ—Ç—Ä–∏–∫–∏...'):
                    progress_bar = st.progress(0)
                    status_text = st.empty()

                    status_text.text('üìÇ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...')
                    progress_bar.progress(0.1)

                    data_transform = transforms.Compose([
                        transforms.Resize(img_size + 32),
                        transforms.CenterCrop(img_size),
                        transforms.ToTensor(),
                        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
                    ])

                    ds = datasets.ImageFolder(test_path, data_transform)
                    loader = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0)

                    total_images = len(ds)
                    expected_batches = (total_images + batch_size - 1) // batch_size
                    status_text.text(f'üìä –ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {total_images}')
                    progress_bar.progress(0.2)

                    status_text.text('üß† –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...')
                    
                    all_labels: List[int] = []
                    all_preds: List[int] = []
                    all_probs: List[np.ndarray] = []

                    model.eval()
                    batch_count = 0
                    
                    with torch.no_grad():
                        for inputs, labels in loader:
                            inputs = inputs.to(device)
                            labels = labels.to(device)
                            logits = model(inputs)
                            probs = torch.softmax(logits, dim=1)
                            preds = torch.argmax(probs, dim=1)

                            all_labels.extend(labels.detach().cpu().numpy().tolist())
                            all_preds.extend(preds.detach().cpu().numpy().tolist())
                            all_probs.extend(probs.detach().cpu().numpy())

                            batch_count += 1
                            progress = 0.2 + (batch_count / len(loader)) * 0.5
                            progress_bar.progress(progress)
                            status_text.text(f'üß† –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {batch_count}/{len(loader)} –±–∞—Ç—á–µ–π...')

                    status_text.text('üìà –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫...')
                    progress_bar.progress(0.8)

                    y_true = np.array(all_labels)
                    y_pred = np.array(all_preds)
                    y_prob = np.array(all_probs)

                    acc = float(np.mean(y_true == y_pred))
                    f1m = float(f1_score(y_true, y_pred, average='macro'))
                    cm = confusion_matrix(y_true, y_pred)
                    rep = classification_report(y_true, y_pred, target_names=CLASSES)

                    y_true_bin = label_binarize(y_true, classes=list(range(len(CLASSES))))
                    roc_auc: Dict[str, float] = {}

                    fig_roc = plt.figure(figsize=(10, 8))
                    for i, cls in enumerate(CLASSES):
                        fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_prob[:, i])
                        roc_auc[cls] = float(auc(fpr, tpr))
                        plt.plot(fpr, tpr, label=f'{cls} (AUC = {roc_auc[cls]:.2f})')
                    plt.plot([0, 1], [0, 1], 'k--')
                    plt.xlim([0.0, 1.0])
                    plt.ylim([0.0, 1.05])
                    plt.xlabel('False Positive Rate')
                    plt.ylabel('True Positive Rate')
                    plt.title('ROC Curve')
                    plt.legend(loc='lower right', fontsize=8)

                    progress_bar.progress(1.0)
                    end_time = pd.Timestamp.now()
                    duration = (end_time - start_time).total_seconds()

                st.success(f"‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ –≤—ã—á–∏—Å–ª–µ–Ω—ã –∑–∞ {duration:.2f} —Å–µ–∫")

                st.markdown("---")
                st.markdown("### üìä –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏")
                
                col_a, col_b, col_c = st.columns(3)
                col_a.metric('Accuracy', f'{acc:.4f}')
                col_b.metric('F1 Macro', f'{f1m:.4f}')
                col_c.metric('Avg ROC AUC', f'{np.mean(list(roc_auc.values())):.4f}')

                col_info1, col_info2 = st.columns(2)
                with col_info1:
                    st.metric("–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", total_images)
                with col_info2:
                    st.metric("–°–∫–æ—Ä–æ—Å—Ç—å", f"{total_images/duration:.1f} img/—Å–µ–∫")

                st.markdown("---")
                st.markdown("### üìà ROC AUC –ø–æ –∫–ª–∞—Å—Å–∞–º")
                
                roc_df = pd.DataFrame([
                    {'–ö–ª–∞—Å—Å': cls, 'ROC AUC': auc_val}
                    for cls, auc_val in roc_auc.items()
                ])
                st.dataframe(roc_df, use_container_width=True, hide_index=True)

                st.markdown("### üéØ ROC Curve")
                st.pyplot(fig_roc)

                st.markdown("### üî• Confusion Matrix")
                fig_cm, ax_cm = plt.subplots(figsize=(10, 8))
                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                           xticklabels=CLASSES, yticklabels=CLASSES, ax=ax_cm)
                ax_cm.set_title('Confusion Matrix')
                ax_cm.set_xlabel('Predicted')
                ax_cm.set_ylabel('Actual')
                st.pyplot(fig_cm)

                st.markdown("### üìã Classification Report")
                st.text(rep)

                st.session_state['last_metrics'] = {
                    'accuracy': acc,
                    'f1_macro': f1m,
                    'roc_auc': roc_auc,
                    'confusion': cm,
                    'report': rep,
                    'fig_roc': fig_roc,
                    'test_path': test_path,
                    'total_images': total_images,
                    'duration': duration
                }

        elif 'last_metrics' in st.session_state:
            st.markdown("---")
            st.markdown("### üìä –ü–æ—Å–ª–µ–¥–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏")
            
            m = st.session_state['last_metrics']
            col_a, col_b, col_c = st.columns(3)
            col_a.metric('Accuracy', f"{m['accuracy']:.4f}")
            col_b.metric('F1 Macro', f"{m['f1_macro']:.4f}")
            col_c.metric('Avg ROC AUC', f"{np.mean(list(m['roc_auc'].values())):.4f}")
            
            st.info(f"–ü–æ—Å–ª–µ–¥–Ω–∏–π —Ç–µ—Å—Ç: {m['test_path']} | –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {m['total_images']} | –í—Ä–µ–º—è: {m['duration']:.2f} —Å–µ–∫")
            
            st.markdown("### üìà ROC AUC –ø–æ –∫–ª–∞—Å—Å–∞–º")
            roc_df = pd.DataFrame([
                {'–ö–ª–∞—Å—Å': cls, 'ROC AUC': auc_val}
                for cls, auc_val in m['roc_auc'].items()
            ])
            st.dataframe(roc_df, use_container_width=True, hide_index=True)

            st.markdown("### üéØ ROC Curve")
            st.pyplot(m['fig_roc'])

            st.markdown("### üî• Confusion Matrix")
            fig_cm, ax_cm = plt.subplots(figsize=(10, 8))
            sns.heatmap(m['confusion'], annot=True, fmt='d', cmap='Blues',
                       xticklabels=CLASSES, yticklabels=CLASSES, ax=ax_cm)
            ax_cm.set_title('Confusion Matrix')
            ax_cm.set_xlabel('Predicted')
            ax_cm.set_ylabel('Actual')
            st.pyplot(fig_cm)

            st.markdown("### üìã Classification Report")
            st.text(m['report'])

        else:
            st.info("üëÜ –ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É '–í—ã—á–∏—Å–ª–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏' –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ")

    with tab_testing:
        st.subheader('–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏')

        st.subheader("üéØ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏")

        col1, col2 = st.columns([2, 1])
        with col1:
            st.write("**–¢–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å:**", os.path.basename(model_path))
        with col2:
            if st.button('üöÄ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö', type='primary', key='test_selected_model'):
                if os.path.exists("test") and os.path.isdir("test"):
                    with st.spinner(f'üîÑ –¢–µ—Å—Ç–∏—Ä—É—é –º–æ–¥–µ–ª—å {os.path.basename(model_path)} –Ω–∞ –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö...'):
                        try:

                            total_images = 0
                            emotion_counts = {}

                            for emotion in CLASSES:
                                emotion_path = os.path.join("test", emotion)
                                if os.path.exists(emotion_path):
                                    count = len([f for f in os.listdir(emotion_path)
                                               if f.lower().endswith(('.png', '.jpg', '.jpeg'))])
                                    emotion_counts[emotion] = count
                                    total_images += count

                            if total_images > 0:
                                st.info(f"üìä –ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {total_images}")
                                st.write("**–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Å–∞–º:**")
                                for emotion, count in emotion_counts.items():
                                    if count > 0:
                                        st.write(f"- {emotion}: {count} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")

                                metrics, test_info = run_model_tests(
                                    model=model,
                                    model_path=model_path,
                                    test_path="test",
                                    device=device,
                                    img_size=img_size,
                                    batch_size=batch_size
                                )

                                st.success("‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

                                roc_auc_avg = np.mean(list(metrics.roc_auc.values())) if metrics.roc_auc else 0.0
                                inference_time = test_info.get('duration_seconds', 0.0)

                                col1, col2, col3, col4 = st.columns(4)
                                with col1:
                                    st.metric("üéØ Accuracy", f"{metrics.accuracy:.4f}")
                                with col2:
                                    st.metric("üìä F1 Macro", f"{metrics.f1_macro:.4f}")
                                with col3:
                                    st.metric("üìà ROC AUC", f"{roc_auc_avg:.4f}")
                                with col4:
                                    st.metric("‚è±Ô∏è –í—Ä–µ–º—è", f"{inference_time:.2f}s")

                                st.subheader("üìã Classification Report")
                                st.text(metrics.report)

                                st.subheader("üî• Confusion Matrix")
                                fig, ax = plt.subplots(figsize=(8, 6))
                                sns.heatmap(metrics.confusion_matrix,
                                          annot=True, fmt='d', cmap='Blues',
                                          xticklabels=CLASSES, yticklabels=CLASSES, ax=ax)
                                ax.set_title('Confusion Matrix')
                                ax.set_xlabel('Predicted')
                                ax.set_ylabel('Actual')
                                st.pyplot(fig)

                                st.subheader("üìà ROC Curves")
                                if hasattr(metrics.fig_roc, 'savefig'):
                                    st.pyplot(metrics.fig_roc)
                                else:
                                    st.info("ROC curves –Ω–µ –¥–æ—Å—Ç—É–ø–Ω—ã")

                            else:
                                st.warning("–ü–∞–ø–∫–∞ test –ø—É—Å—Ç–∞ –∏–ª–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")

                        except Exception as e:
                            st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
                            import traceback
                            st.error(f"–î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏: {traceback.format_exc()}")
                else:
                    st.error("–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ø–∞–ø–∫–∞ test –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

        st.divider()

        if st.button('–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π test', key='check_standard_test'):
            if os.path.exists("test") and os.path.isdir("test"):
                st.info("–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä –Ω–∞–π–¥–µ–Ω –∏ –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")

                total_images = 0
                emotion_counts = {}

                for emotion in CLASSES:
                    emotion_path = os.path.join("test", emotion)
                    if os.path.exists(emotion_path):
                        count = len([f for f in os.listdir(emotion_path)
                                   if f.lower().endswith(('.png', '.jpg', '.jpeg'))])
                        emotion_counts[emotion] = count
                        total_images += count

                if total_images > 0:
                    st.write(f"**–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ test:** {total_images}")
                    st.write("**–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Å–∞–º:**")
                    for emotion, count in emotion_counts.items():
                        if count > 0:
                            st.write(f"- {emotion}: {count} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
                else:
                    st.warning("–ü–∞–ø–∫–∞ test –ø—É—Å—Ç–∞ –∏–ª–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
            else:
                st.error("–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ø–∞–ø–∫–∞ test –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

        st.divider()

        st.subheader("–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π")

        if st.button('üöÄ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ –º–æ–¥–µ–ª–∏', type='primary', key='test_all_models'):
            available_models = get_available_models()

            if available_models:
                all_results, all_test_infos = test_all_models(
                    available_models=available_models,
                    test_path=test_path,
                    device=device,
                    img_size=img_size,
                    batch_size=batch_size
                )

                if all_results:
                    st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π")

                    df = pd.DataFrame(all_results)

                    df = df.sort_values('accuracy', ascending=False)
                    df['rank'] = range(1, len(df) + 1)

                    winner = df.iloc[0]

                    def get_rank_color(rank):
                        if rank == 1:
                            return "ü•á"
                        elif rank == 2:
                            return "ü•à"
                        elif rank == 3:
                            return "ü•â"
                        else:
                            return f"#{rank}"

                    df['rank_emoji'] = df['rank'].apply(get_rank_color)

                    display_df = df[['rank_emoji', 'model', 'accuracy', 'f1_macro', 'roc_auc_avg']].copy()
                    display_df.columns = ['–†–∞–Ω–≥', '–ú–æ–¥–µ–ª—å', 'Accuracy', 'F1 Macro', 'Avg ROC AUC']

                    st.dataframe(display_df, use_container_width=True)

                    st.success(f"üèÜ **–ü–æ–±–µ–¥–∏—Ç–µ–ª—å:** {winner['model']} (Accuracy: {winner['accuracy']:.4f}, F1: {winner['f1_macro']:.4f})")

                    st.subheader("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º")
                    fig, ax = plt.subplots(figsize=(12, 7))

                    models_names = df['model'].tolist()
                    accuracies = df['accuracy'].tolist()
                    f1_scores = df['f1_macro'].tolist()
                    ranks = df['rank'].tolist()

                    x = np.arange(len(models_names))
                    width = 0.6

                    colors_list = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(models_names)))

                    bars = ax.bar(x, accuracies, width, label='Accuracy', color=colors_list)

                    for i, (rank, acc, model_name) in enumerate(zip(ranks, accuracies, models_names)):
                        ax.text(i + width/2, acc + 0.01, f'#{rank}',
                                ha='center', va='bottom', fontweight='bold', fontsize=8)
                        ax.text(i + width/2, acc - 0.02, f'{acc:.3f}',
                                ha='center', va='top', fontsize=7)

                    ax.set_xlabel('–ú–æ–¥–µ–ª–∏ (—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω—ã –ø–æ Accuracy)')
                    ax.set_ylabel('Accuracy')
                    ax.set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π (ü•á = –ª—É—á—à–∞—è)')
                    ax.set_xticks(x)
                    ax.set_xticklabels(models_names, rotation=45, ha='right')
                    ax.legend()
                    ax.grid(True, alpha=0.3)

                    rank_colors = plt.cm.RdYlGn(np.linspace(0.8, 0.2, len(ranks)))
                    sm = plt.cm.ScalarMappable(cmap=plt.cm.RdYlGn)
                    sm.set_array(np.array(ranks))
                    cbar = plt.colorbar(sm, ax=ax, orientation='vertical', fraction=0.02, pad=0.04)
                    cbar.set_label('–†–∞–Ω–≥ (1=–ª—É—á—à–∏–π)', rotation=270, va='bottom')
                    cbar.set_ticks(ranks)

                    st.pyplot(fig)

                    st.subheader("üìã –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –º–æ–¥–µ–ª—è–º")

                    for idx, row in df.iterrows():
                        with st.expander(f"üèÜ #{row['rank']} {row['model']} - Accuracy: {row['accuracy']:.4f}"):
                            col1, col2, col3 = st.columns(3)

                            with col1:
                                st.metric("Accuracy", f"{row['accuracy']:.4f}")
                                st.metric("F1 Macro", f"{row['f1_macro']:.4f}")

                            with col2:
                                st.metric("Avg ROC AUC", f"{row['roc_auc_avg']:.4f}")
                                st.metric("–†–∞–Ω–≥", f"#{row['rank']}")

                            with col3:
                                if row['rank'] == 1:
                                    st.success("ü•á –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å")
                                elif row['rank'] == 2:
                                    st.info("ü•à –í—Ç–æ—Ä–æ–µ –º–µ—Å—Ç–æ")
                                elif row['rank'] == 3:
                                    st.warning("ü•â –¢—Ä–µ—Ç—å–µ –º–µ—Å—Ç–æ")
                                else:
                                    st.write(f"–ú–µ—Å—Ç–æ: #{row['rank']}")

                            if 'roc_auc' in row and isinstance(row['roc_auc'], dict):
                                st.write("**ROC AUC –ø–æ –∫–ª–∞—Å—Å–∞–º:**")
                                roc_auc_df = pd.DataFrame([
                                    {'–ö–ª–∞—Å—Å': k, 'ROC AUC': f"{v:.4f}"}
                                    for k, v in row['roc_auc'].items()
                                ])
                                st.dataframe(roc_auc_df, use_container_width=True)

                    st.divider()

                    st.subheader("üíæ –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                    col_export1, col_export2, col_export3, col_export4 = st.columns(4)

                    with col_export1:

                        detailed_csv_data = []

                        headers = [
                            'Rank', 'Model', 'Accuracy', 'F1_Macro', 'Avg_ROC_AUC',
                            'ROC_AUC_angry', 'ROC_AUC_disgust', 'ROC_AUC_fear', 'ROC_AUC_happy',
                            'ROC_AUC_neutral', 'ROC_AUC_sad', 'ROC_AUC_surprise',
                            'Precision_angry', 'Recall_angry', 'F1_angry',
                            'Precision_disgust', 'Recall_disgust', 'F1_disgust',
                            'Precision_fear', 'Recall_fear', 'F1_fear',
                            'Precision_happy', 'Recall_happy', 'F1_happy',
                            'Precision_neutral', 'Recall_neutral', 'F1_neutral',
                            'Precision_sad', 'Recall_sad', 'F1_sad',
                            'Precision_surprise', 'Recall_surprise', 'F1_surprise',
                            'True_angry', 'True_disgust', 'True_fear', 'True_happy',
                            'True_neutral', 'True_sad', 'True_surprise',
                            'Pred_angry', 'Pred_disgust', 'Pred_fear', 'Pred_happy',
                            'Pred_neutral', 'Pred_sad', 'Pred_surprise',
                            'Support_angry', 'Support_disgust', 'Support_fear', 'Support_happy',
                            'Support_neutral', 'Support_sad', 'Support_surprise',
                            'Error_Status'
                        ]
                        detailed_csv_data.append(','.join(headers))

                        for idx, row in df.iterrows():
                            model_result = all_results[idx]
                            model_name = row['model']

                            base_data = [
                                str(row['rank']),
                                f'"{model_name}"',
                                f"{row['accuracy']:.6f}",
                                f"{row['f1_macro']:.6f}",
                                f"{row['roc_auc_avg']:.6f}"
                            ]

                            roc_auc_data = []
                            if 'roc_auc' in model_result and isinstance(model_result['roc_auc'], dict):
                                for cls in CLASSES:
                                    roc_auc_data.append(f"{model_result['roc_auc'].get(cls, 0):.6f}")
                            else:
                                roc_auc_data = ['0.000000'] * len(CLASSES)

                            precision_data = []
                            recall_data = []
                            f1_data = []
                            support_data = []

                            if 'report' in model_result and model_result['report']:
                                report_lines = model_result['report'].split('\n')
                                for line in report_lines:
                                    if any(cls in line for cls in CLASSES):
                                        parts = line.split()
                                        if len(parts) >= 4:
                                            try:

                                                cls_name = None
                                                for cls in CLASSES:
                                                    if line.strip().startswith(cls):
                                                        cls_name = cls
                                                        break

                                                if cls_name:

                                                    precision = float(parts[-4])
                                                    recall = float(parts[-3])
                                                    f1 = float(parts[-2])
                                                    support = int(parts[-1])

                                                    precision_data.append(f"{precision:.6f}")
                                                    recall_data.append(f"{recall:.6f}")
                                                    f1_data.append(f"{f1:.6f}")
                                                    support_data.append(str(support))
                                            except (ValueError, IndexError):

                                                precision_data.append("0.000000")
                                                recall_data.append("0.000000")
                                                f1_data.append("0.000000")
                                                support_data.append("0")

                            while len(precision_data) < len(CLASSES):
                                precision_data.append("0.000000")
                                recall_data.append("0.000000")
                                f1_data.append("0.000000")
                                support_data.append("0")

                            confusion_data = []
                            if 'confusion' in model_result and model_result['confusion'] is not None:
                                cm = model_result['confusion']
                                if isinstance(cm, np.ndarray):

                                    for i in range(len(CLASSES)):
                                        confusion_data.append(str(int(cm[i].sum())))

                                    for j in range(len(CLASSES)):
                                        confusion_data.append(str(int(cm[:, j].sum())))
                            else:
                                confusion_data = ['0'] * (len(CLASSES) * 2)

                            error_status = model_result.get('error', 'Success')
                            if error_status:
                                error_status = f'"{error_status}"'
                            else:
                                error_status = 'Success'

                            full_row = base_data + roc_auc_data + precision_data + recall_data + f1_data + confusion_data + support_data + [error_status]
                            detailed_csv_data.append(','.join(full_row))

                        detailed_csv_text = '\n'.join(detailed_csv_data)

                        st.download_button(
                            label="üìä –°—É–ø–µ—Ä –¥–µ—Ç–∞–ª—å–Ω—ã–π CSV",
                            data=detailed_csv_text,
                            file_name="models_super_detailed_report.csv",
                            mime="text/csv"
                        )

                    with col_export2:

                        confusion_csv_data = []

                        confusion_headers = ['Model', 'Rank', 'True_Class', 'Pred_Class', 'Count']
                        confusion_csv_data.append(','.join(confusion_headers))

                        for idx, row in df.iterrows():
                            model_result = all_results[idx]
                            model_name = f'"{row["model"]}"'
                            rank = str(row['rank'])

                            if 'confusion' in model_result and model_result['confusion'] is not None:
                                cm = model_result['confusion']
                                if isinstance(cm, np.ndarray):
                                    for i, true_class in enumerate(CLASSES):
                                        for j, pred_class in enumerate(CLASSES):
                                            count = int(cm[i][j])
                                            if count > 0:
                                                confusion_row = [
                                                    model_name,
                                                    rank,
                                                    true_class,
                                                    pred_class,
                                                    str(count)
                                                ]
                                                confusion_csv_data.append(','.join(confusion_row))

                        confusion_csv_text = '\n'.join(confusion_csv_data)

                        st.download_button(
                            label="üéØ Confusion Matrix CSV",
                            data=confusion_csv_text,
                            file_name="models_confusion_matrices.csv",
                            mime="text/csv"
                        )

                    with col_export3:

                        if st.button("üìà ROC –∫—Ä–∏–≤—ã–µ (ZIP)"):
                            import zipfile
                            import io

                            zip_buffer = io.BytesIO()

                            with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                                for idx, row in df.iterrows():
                                    model_result = all_results[idx]
                                    model_name = row['model'].replace('.pth', '')

                                    if 'fig_roc' in model_result and model_result['fig_roc']:

                                        img_buffer = io.BytesIO()
                                        model_result['fig_roc'].savefig(img_buffer, format='png', dpi=150, bbox_inches='tight')
                                        img_buffer.seek(0)

                                        zip_file.writestr(f"roc_curve_{model_name}.png", img_buffer.getvalue())

                                        metadata = f
                                        if 'roc_auc' in model_result and isinstance(model_result['roc_auc'], dict):
                                            for cls, auc_val in model_result['roc_auc'].items():
                                                metadata += f"{cls}: {auc_val:.6f}\n"

                                        zip_file.writestr(f"roc_metadata_{model_name}.txt", metadata)

                            zip_buffer.seek(0)

                            st.download_button(
                                label="üìà –°–∫–∞—á–∞—Ç—å ROC –∫—Ä–∏–≤—ã–µ (ZIP)",
                                data=zip_buffer.getvalue(),
                                file_name="roc_curves.zip",
                                mime="application/zip"
                            )

                    with col_export4:

                        combined_csv_data = []

                        combined_csv_data.append("=== –°–í–û–î–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===")
                        combined_csv_data.append("Rank,Model,Accuracy,F1_Macro,Avg_ROC_AUC")
                        for idx, row in df.iterrows():
                            combined_csv_data.append(f"{row['rank']},\"{row['model']}\",{row['accuracy']:.6f},{row['f1_macro']:.6f},{row['roc_auc_avg']:.6f}")

                        combined_csv_data.append("\n=== –î–ï–¢–ê–õ–¨–ù–´–ï –ú–ï–¢–†–ò–ö–ò –ü–û –ö–õ–ê–°–°–ê–ú ===")

                        class_headers = ['Model', 'Class', 'Precision', 'Recall', 'F1_Score', 'Support', 'ROC_AUC']
                        combined_csv_data.append(','.join(class_headers))

                        for idx, row in df.iterrows():
                            model_result = all_results[idx]
                            model_name = f'"{row["model"]}"'

                            if 'report' in model_result and model_result['report']:
                                report_lines = model_result['report'].split('\n')
                                for line in report_lines:
                                    if any(cls in line for cls in CLASSES):
                                        parts = line.split()
                                        if len(parts) >= 4:
                                            try:

                                                cls_name = None
                                                for cls in CLASSES:
                                                    if line.strip().startswith(cls):
                                                        cls_name = cls
                                                        break

                                                if cls_name:
                                                    precision = float(parts[-4])
                                                    recall = float(parts[-3])
                                                    f1 = float(parts[-2])
                                                    support = int(parts[-1])
                                                    roc_auc = model_result.get('roc_auc', {}).get(cls_name, 0)

                                                    class_row = [
                                                        model_name,
                                                        cls_name,
                                                        f"{precision:.6f}",
                                                        f"{recall:.6f}",
                                                        f"{f1:.6f}",
                                                        str(support),
                                                        f"{roc_auc:.6f}"
                                                    ]
                                                    combined_csv_data.append(','.join(class_row))
                                            except (ValueError, IndexError):
                                                pass

                        combined_csv_data.append("\n=== –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –í–°–ï–ú –ú–û–î–ï–õ–Ø–ú ===")
                        accuracies = df['accuracy'].tolist()
                        f1_scores = df['f1_macro'].tolist()
                        roc_aucs = df['roc_auc_avg'].tolist()

                        stats_data = [
                            f"Metric,Mean,Std,Min,Max",
                            f"Accuracy,{np.mean(accuracies):.6f},{np.std(accuracies):.6f},{np.min(accuracies):.6f},{np.max(accuracies):.6f}",
                            f"F1_Macro,{np.mean(f1_scores):.6f},{np.std(f1_scores):.6f},{np.min(f1_scores):.6f},{np.max(f1_scores):.6f}",
                            f"ROC_AUC,{np.mean(roc_aucs):.6f},{np.std(roc_aucs):.6f},{np.min(roc_aucs):.6f},{np.max(roc_aucs):.6f}"
                        ]
                        combined_csv_data.extend(stats_data)

                        combined_csv_text = '\n'.join(combined_csv_data)

                        st.download_button(
                            label="üìã –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á–µ—Ç (CSV)",
                            data=combined_csv_text,
                            file_name="models_combined_report.csv",
                            mime="text/csv"
                        )

                    with col_export3:

                        detailed_report = []
                        detailed_report.append("=== –°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –ú–û–î–ï–õ–ï–ô ===\n")
                        detailed_report.append(f"–¢–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä: {test_path}")
                        detailed_report.append(f"–†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {img_size}x{img_size}")
                        detailed_report.append(f"Batch size: {batch_size}")
                        detailed_report.append(f"–í—Å–µ–≥–æ –º–æ–¥–µ–ª–µ–π: {len(df)}")
                        detailed_report.append(f"–î–∞—Ç–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

                        detailed_report.append("=== –†–ê–ù–ñ–ò–†–û–í–ê–ù–ò–ï ===")
                        for idx, row in df.iterrows():
                            detailed_report.append(f"#{row['rank']} {row['rank_emoji']} {row['model']}")
                            detailed_report.append(f"  Accuracy: {row['accuracy']:.4f}")
                            detailed_report.append(f"  F1 Macro: {row['f1_macro']:.4f}")
                            detailed_report.append(f"  Avg ROC AUC: {row['roc_auc_avg']:.4f}\n")

                        detailed_report.append("=== –ü–û–ë–ï–î–ò–¢–ï–õ–¨ ===")
                        winner = df.iloc[0]
                        detailed_report.append(f"üèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {winner['model']}")
                        detailed_report.append(f"   Accuracy: {winner['accuracy']:.4f}")
                        detailed_report.append(f"   F1 Macro: {winner['f1_macro']:.4f}")
                        detailed_report.append(f"   Avg ROC AUC: {winner['roc_auc_avg']:.4f}")

                        report_text = "\n".join(detailed_report)

                        st.download_button(
                            label="üìÑ –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç (TXT)",
                            data=report_text,
                            file_name="models_comparison_report.txt",
                            mime="text/plain"
                        )

                    with col_export1:

                        super_detailed_report = []
                        super_detailed_report.append("=" * 80)
                        super_detailed_report.append("–°–£–ü–ï–† –î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –ü–û –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Æ –ú–û–î–ï–õ–ï–ô")
                        super_detailed_report.append("=" * 80)
                        super_detailed_report.append("")

                        super_detailed_report.append("–û–ë–©–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø")
                        super_detailed_report.append("-" * 40)
                        super_detailed_report.append(f"–¢–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä: {test_path}")
                        super_detailed_report.append(f"–†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {img_size}x{img_size}")
                        super_detailed_report.append(f"Batch size: {batch_size}")
                        super_detailed_report.append(f"–í—Å–µ–≥–æ –º–æ–¥–µ–ª–µ–π –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ: {len(df)}")
                        super_detailed_report.append(f"–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
                        super_detailed_report.append(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
                        super_detailed_report.append("")

                        super_detailed_report.append("–°–í–û–î–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
                        super_detailed_report.append("-" * 40)
                        super_detailed_report.append(f"{'–†–∞–Ω–≥':<5} {'–ú–æ–¥–µ–ª—å':<30} {'Accuracy':<12} {'F1 Macro':<12} {'ROC AUC':<12}")
                        super_detailed_report.append("-" * 80)
                        for idx, row in df.iterrows():
                            super_detailed_report.append(
                                f"#{row['rank']:<4} {row['model'][:29]:<30} {row['accuracy']:<12.4f} "
                                f"{row['f1_macro']:<12.4f} {row['roc_auc_avg']:<12.4f}"
                            )
                        super_detailed_report.append("")

                        super_detailed_report.append("–î–ï–¢–ê–õ–¨–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –ü–û –ö–ê–ñ–î–û–ô –ú–û–î–ï–õ–ò")
                        super_detailed_report.append("=" * 80)

                        for idx, row in df.iterrows():
                            model_result = all_results[idx]
                            super_detailed_report.append("")
                            super_detailed_report.append(f"–ú–û–î–ï–õ–¨ #{row['rank']}: {row['model']}")
                            super_detailed_report.append("=" * 50)

                            super_detailed_report.append("–û–°–ù–û–í–ù–´–ï –ú–ï–¢–†–ò–ö–ò:")
                            super_detailed_report.append(f"  Accuracy: {row['accuracy']:.6f}")
                            super_detailed_report.append(f"  F1 Macro: {row['f1_macro']:.6f}")
                            super_detailed_report.append(f"  Average ROC AUC: {row['roc_auc_avg']:.6f}")

                            if 'roc_auc' in model_result and isinstance(model_result['roc_auc'], dict):
                                super_detailed_report.append("")
                                super_detailed_report.append("ROC AUC –ü–û –ö–õ–ê–°–°–ê–ú:")
                                for class_name, roc_auc_value in model_result['roc_auc'].items():
                                    super_detailed_report.append(f"  {class_name:<12}: {roc_auc_value:.6f}")

                            if 'report' in model_result and model_result['report']:
                                super_detailed_report.append("")
                                super_detailed_report.append("CLASSIFICATION REPORT:")
                                super_detailed_report.append("-" * 30)
                                report_lines = model_result['report'].split('\n')
                                for line in report_lines:
                                    if line.strip():
                                        super_detailed_report.append(f"  {line}")

                            if 'confusion' in model_result and model_result['confusion'] is not None:
                                super_detailed_report.append("")
                                super_detailed_report.append("CONFUSION MATRIX:")
                                super_detailed_report.append("-" * 20)
                                cm = model_result['confusion']
                                if isinstance(cm, np.ndarray):

                                    header = "        " + "  ".join([f"{cls:>8}" for cls in CLASSES])
                                    super_detailed_report.append(header)
                                    super_detailed_report.append("        " + "-" * (8 * len(CLASSES) + 2 * (len(CLASSES) - 1)))

                                    for i, true_class in enumerate(CLASSES):
                                        row_str = f"{true_class:>8}: " + "  ".join([f"{cm[i][j]:>8}" for j in range(len(CLASSES))])
                                        super_detailed_report.append(row_str)

                            super_detailed_report.append("")
                            super_detailed_report.append("–î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
                            if 'error' in model_result and model_result['error']:
                                super_detailed_report.append(f"  –û—à–∏–±–∫–∏ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {model_result['error']}")
                            else:
                                super_detailed_report.append("  –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–π–¥–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")

                            if row['rank'] == 1:
                                super_detailed_report.append("  ü•á –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨ –í –†–ï–ô–¢–ò–ù–ì–ï")
                            elif row['rank'] == 2:
                                super_detailed_report.append("  ü•à –í–¢–û–†–û–ï –ú–ï–°–¢–û")
                            elif row['rank'] == 3:
                                super_detailed_report.append("  ü•â –¢–†–ï–¢–¨–ï –ú–ï–°–¢–û")
                            else:
                                super_detailed_report.append(f"  –ú–µ—Å—Ç–æ –≤ —Ä–µ–π—Ç–∏–Ω–≥–µ: #{row['rank']}")

                            super_detailed_report.append("")
                            super_detailed_report.append("~" * 50)

                        super_detailed_report.append("")
                        super_detailed_report.append("–ò–¢–û–ì–û–í–´–ï –í–´–í–û–î–´")
                        super_detailed_report.append("=" * 50)
                        super_detailed_report.append(f"üèÜ –ü–æ–±–µ–¥–∏—Ç–µ–ª—å —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {winner['model']}")
                        super_detailed_report.append(f"   –õ—É—á—à–∏–π Accuracy: {winner['accuracy']:.6f}")
                        super_detailed_report.append(f"   –õ—É—á—à–∏–π F1 Macro: {winner['f1_macro']:.6f}")
                        super_detailed_report.append(f"   –õ—É—á—à–∏–π Avg ROC AUC: {winner['roc_auc_avg']:.6f}")

                        accuracies = df['accuracy'].tolist()
                        f1_scores = df['f1_macro'].tolist()
                        roc_aucs = df['roc_auc_avg'].tolist()

                        super_detailed_report.append("")
                        super_detailed_report.append("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –í–°–ï–ú –ú–û–î–ï–õ–Ø–ú:")
                        super_detailed_report.append(f"  –°—Ä–µ–¥–Ω–∏–π Accuracy: {np.mean(accuracies):.6f} ¬± {np.std(accuracies):.6f}")
                        super_detailed_report.append(f"  –°—Ä–µ–¥–Ω–∏–π F1 Macro: {np.mean(f1_scores):.6f} ¬± {np.std(f1_scores):.6f}")
                        super_detailed_report.append(f"  –°—Ä–µ–¥–Ω–∏–π ROC AUC: {np.mean(roc_aucs):.6f} ¬± {np.std(roc_aucs):.6f}")
                        super_detailed_report.append(f"  –†–∞–∑–±—Ä–æ—Å Accuracy: {np.max(accuracies) - np.min(accuracies):.6f}")
                        super_detailed_report.append(f"  –†–∞–∑–±—Ä–æ—Å F1 Macro: {np.max(f1_scores) - np.min(f1_scores):.6f}")

                        super_detailed_report.append("")
                        super_detailed_report.append("=" * 80)
                        super_detailed_report.append("–ö–û–ù–ï–¶ –û–¢–ß–ï–¢–ê")
                        super_detailed_report.append("=" * 80)

                        super_report_text = "\n".join(super_detailed_report)

                        st.download_button(
                            label="üìã –°—É–ø–µ—Ä –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç (TXT)",
                            data=super_report_text,
                            file_name="models_super_detailed_report.txt",
                            mime="text/plain"
                        )

                    with col_export2:

                        json_export = {
                            "test_info": {
                                "test_path": test_path,
                                "img_size": img_size,
                                "batch_size": batch_size,
                                "device": str(device),
                                "timestamp": pd.Timestamp.now().isoformat(),
                                "total_models": len(df)
                            },
                            "ranking": {
                                "winner": {
                                    "model": winner['model'],
                                    "rank": 1,
                                    "accuracy": winner['accuracy'],
                                    "f1_macro": winner['f1_macro'],
                                    "roc_auc_avg": winner['roc_auc_avg']
                                },
                                "all_models": df.to_dict('records')
                            },
                            "detailed_results": [],
                            "test_infos": [],
                            "statistics": {
                                "accuracy": {
                                    "mean": float(np.mean(accuracies)),
                                    "std": float(np.std(accuracies)),
                                    "min": float(np.min(accuracies)),
                                    "max": float(np.max(accuracies))
                                },
                                "f1_macro": {
                                    "mean": float(np.mean(f1_scores)),
                                    "std": float(np.std(f1_scores)),
                                    "min": float(np.min(f1_scores)),
                                    "max": float(np.max(f1_scores))
                                },
                                "roc_auc_avg": {
                                    "mean": float(np.mean(roc_aucs)),
                                    "std": float(np.std(roc_aucs)),
                                    "min": float(np.min(roc_aucs)),
                                    "max": float(np.max(roc_aucs))
                                }
                            }
                        }

                        for result in all_results:
                            result_copy = result.copy()

                            if 'fig_roc' in result_copy:
                                del result_copy['fig_roc']
                            if 'test_info' in result_copy and result_copy['test_info']:
                                test_info_copy = result_copy['test_info'].copy()

                                if 'start_time' in test_info_copy:
                                    test_info_copy['start_time'] = str(test_info_copy['start_time'])
                                if 'end_time' in test_info_copy:
                                    test_info_copy['end_time'] = str(test_info_copy['end_time'])
                                result_copy['test_info'] = test_info_copy
                            json_export["detailed_results"].append(result_copy)

                        for test_info in all_test_infos:
                            if test_info:
                                test_info_copy = test_info.copy()

                                if 'start_time' in test_info_copy:
                                    test_info_copy['start_time'] = str(test_info_copy['start_time'])
                                if 'end_time' in test_info_copy:
                                    test_info_copy['end_time'] = str(test_info_copy['end_time'])
                                json_export["test_infos"].append(test_info_copy)

                        def convert_numpy(obj):
                            try:
                                if isinstance(obj, np.ndarray):
                                    return obj.tolist()
                                elif isinstance(obj, np.integer):
                                    return int(obj)
                                elif isinstance(obj, np.floating):
                                    return float(obj)
                                elif isinstance(obj, dict):
                                    return {key: convert_numpy(value) for key, value in obj.items()}
                                elif isinstance(obj, list):
                                    return [convert_numpy(item) for item in obj]
                                elif hasattr(obj, '__class__'):

                                    class_name = obj.__class__.__name__
                                    if 'Figure' in class_name:
                                        return f"[Figure object - {class_name}]"
                                    elif 'Timestamp' in class_name:
                                        return str(obj)
                                    elif 'Axes' in class_name:
                                        return f"[Axes object - {class_name}]"
                                    elif 'Canvas' in class_name:
                                        return f"[Canvas object - {class_name}]"
                                    else:

                                        return str(obj)
                                else:
                                    return obj
                            except Exception as e:

                                return f"[Conversion error: {str(obj)}]"

                        json_export_clean = convert_numpy(json_export)
                        json_data = json.dumps(json_export_clean, indent=2, ensure_ascii=False)

                        st.download_button(
                            label="üìÑ –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç (JSON)",
                            data=json_data,
                            file_name="models_complete_report.json",
                            mime="application/json"
                        )

                    st.session_state.last_test_results = all_results
                    st.session_state.last_test_df = df
                    st.session_state.last_test_winner = winner
                    st.session_state.last_test_infos = all_test_infos

                    st.session_state.last_test_path = test_path
                    st.session_state.last_img_size = img_size
                    st.session_state.last_batch_size = batch_size
                    st.session_state.last_device = device

                    try:
                        with st.spinner("üîÑ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ ZIP-–æ—Ç—á–µ—Ç–∞..."):
                            zip_data = create_complete_zip_report(
                                all_results, df, winner, all_test_infos,
                                test_path, img_size, batch_size, device
                            )

                            os.makedirs('logs', exist_ok=True)
                            zip_filename = f"logs/models_complete_report_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.zip"

                            with open(zip_filename, 'wb') as f:
                                f.write(zip_data)

                            st.success(f"üì¶ ZIP-–æ—Ç—á–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {zip_filename}")
                            st.info(f"üìÅ –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {len(zip_data)} –±–∞–π—Ç")
                            st.info(f"üìÇ –§–∞–π–ª –¥–æ—Å—Ç—É–ø–µ–Ω –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {os.path.abspath(zip_filename)}")
                    except Exception as e:
                        st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–º —Å–æ–∑–¥–∞–Ω–∏–∏ ZIP-–æ—Ç—á–µ—Ç–∞: {e}")
                        import traceback
                        st.error(f"–î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏: {traceback.format_exc()}")

                st.divider()
                st.subheader("üóÇÔ∏è –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç")
                st.markdown("–°–∫–∞—á–∞–π—Ç–µ –ø–æ–ª–Ω—ã–π ZIP-–∞—Ä—Ö–∏–≤ —Å–æ –≤—Å–µ–º–∏ –æ—Ç—á–µ—Ç–∞–º–∏ –∏ –¥–∞–Ω–Ω—ã–º–∏")

                if 'last_test_results' in st.session_state and st.session_state.last_test_results:
                    st.success("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–æ—Å—Ç—É–ø–Ω—ã –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è")

                    if st.button("üì¶ –°–æ–∑–¥–∞—Ç—å –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å ZIP-–æ—Ç—á–µ—Ç", type="primary", use_container_width=True):
                        try:
                            with st.spinner("üîÑ –°–æ–∑–¥–∞–Ω–∏–µ ZIP-–∞—Ä—Ö–∏–≤–∞..."):

                                all_results = st.session_state.last_test_results
                                df = st.session_state.last_test_df
                                winner = st.session_state.last_test_winner
                                all_test_infos = st.session_state.last_test_infos
                                test_path = st.session_state.last_test_path
                                img_size = st.session_state.last_img_size
                                batch_size = st.session_state.last_batch_size
                                device = st.session_state.last_device

                                zip_data = create_complete_zip_report(
                                    all_results, df, winner, all_test_infos,
                                    test_path, img_size, batch_size, device
                                )

                                os.makedirs('logs', exist_ok=True)
                                zip_filename = f"logs/models_complete_report_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.zip"

                                with open(zip_filename, 'wb') as f:
                                    f.write(zip_data)

                                st.success(f"üì¶ ZIP-–æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {zip_filename}")
                                st.info(f"üìÅ –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {len(zip_data)} –±–∞–π—Ç")
                                st.info(f"üìÇ –§–∞–π–ª –¥–æ—Å—Ç—É–ø–µ–Ω –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {os.path.abspath(zip_filename)}")

                        except Exception as e:
                            st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ ZIP-–æ—Ç—á–µ—Ç–∞: {e}")
                            import traceback
                            st.error(f"–î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏: {traceback.format_exc()}")
                else:
                    st.info("üìã –î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ—Ç—á–µ—Ç–∞ —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ–¥–∏—Ç–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π")

    with tab_camera:
        st.subheader('üì∑ –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —ç–º–æ—Ü–∏–π —Å –∫–∞–º–µ—Ä—ã')

        st.write()

        source_option = st.radio(
            "–í—ã–±–µ—Ä–∏—Ç–µ –∏—Å—Ç–æ—á–Ω–∏–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:",
            ["üì∏ –°–¥–µ–ª–∞—Ç—å —Å–Ω–∏–º–æ–∫", "üé• –ñ–∏–≤–∞—è –∫–∞–º–µ—Ä–∞"],
            horizontal=True
        )

        col1, col2 = st.columns(2)
        with col1:
            camera_scale_factor = st.slider('–ú–∞—Å—à—Ç–∞–± (–¥–µ—Ç–µ–∫—Ü–∏—è –ª–∏—Ü)', min_value=1.05, max_value=1.50, value=1.10, step=0.01)
        with col2:
            camera_min_neighbors = st.slider('–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Å–æ—Å–µ–¥–∏', min_value=3, max_value=10, value=5, step=1)

        if source_option == "üé• –ñ–∏–≤–∞—è –∫–∞–º–µ—Ä–∞":
            col3, col4 = st.columns(2)
            with col3:
                fps_limit = st.slider('FPS –ª–∏–º–∏—Ç', min_value=1, max_value=30, value=10, step=1)
            with col4:
                show_confidence = st.toggle('–ü–æ–∫–∞–∑—ã–≤–∞—Ç—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å', value=True)

        face_cascade = get_face_cascade()

        if source_option == "üé• –ñ–∏–≤–∞—è –∫–∞–º–µ—Ä–∞":

            st.info("üé• –ù–∞–∂–º–∏—Ç–µ '–ó–∞–ø—É—Å—Ç–∏—Ç—å –∂–∏–≤—É—é –∫–∞–º–µ—Ä—É' –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏")

            if st.button("üé• –ó–∞–ø—É—Å—Ç–∏—Ç—å –∂–∏–≤—É—é –∫–∞–º–µ—Ä–∞", type="primary"):

                video_placeholder = st.empty()
                stats_placeholder = st.empty()

                cap = cv2.VideoCapture(0)
                if not cap.isOpened():
                    st.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –∫–∞–º–µ—Ä—É. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∫–∞–º–µ—Ä–∞ –¥–æ—Å—Ç—É–ø–Ω–∞.")
                    st.stop()

                st.info("üî¥ –ö–∞–º–µ—Ä–∞ –∑–∞–ø—É—â–µ–Ω–∞. –ù–∞–∂–º–∏—Ç–µ '–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è.")

                stop_button = st.button("üõë –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∫–∞–º–µ—Ä—É")

                frame_count = 0
                start_time = time.time()

                while not stop_button and cap.isOpened():
                    ret, frame = cap.read()
                    if not ret:
                        break

                    frame_count += 1
                    if frame_count % (30 // fps_limit) != 0:
                        continue

                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

                    faces = face_cascade.detectMultiScale(
                        gray,
                        scaleFactor=camera_scale_factor,
                        minNeighbors=camera_min_neighbors,
                        minSize=(30, 30)
                    )

                    results = []
                    for (x, y, w, h) in faces:
                        face_roi = frame[y:y+h, x:x+w]

                        probs = predict_on_bgr_image(face_roi, model, device, img_size)
                        emotion_idx = np.argmax(probs)
                        emotion = CLASSES[emotion_idx]
                        confidence = float(probs[emotion_idx])

                        results.append({
                            'emotion': emotion,
                            'confidence': confidence,
                            'position': (x, y, w, h)
                        })

                        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)

                        if show_confidence:
                            label = f'{emotion}: {confidence:.0%}'
                        else:
                            label = emotion

                        cv2.putText(frame, label, (x, y-10),
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)

                    video_placeholder.image(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),
                                         channels="RGB", use_container_width=True)

                    if results:
                        emotion_counts = {}
                        for result in results:
                            emotion = result['emotion']
                            emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1

                        stats_text = f"üéØ –ù–∞–π–¥–µ–Ω–æ –ª–∏—Ü: {len(results)} | "
                        stats_text += " | ".join([f"{emo}: {count}" for emo, count in emotion_counts.items()])

                        stats_placeholder.info(stats_text)
                    else:
                        stats_placeholder.warning("üòî –õ–∏—Ü–∞ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã")

                    time.sleep(0.01)

                cap.release()
                video_placeholder.empty()
                stats_placeholder.empty()
                st.success("‚úÖ –ö–∞–º–µ—Ä–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")

        elif source_option == "üì∏ –°–¥–µ–ª–∞—Ç—å —Å–Ω–∏–º–æ–∫":

            image_file = st.camera_input("–°–¥–µ–ª–∞–π—Ç–µ —Å–Ω–∏–º–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —ç–º–æ—Ü–∏–π")

            if image_file is not None:
                process_single_image(image_file, face_cascade, camera_scale_factor,
                                   camera_min_neighbors, model, device, img_size)

        with st.expander("üìñ –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é"):
            st.write("""
            **–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–º–µ—Ä—É –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —ç–º–æ—Ü–∏–π:**

            **üì∏ –°–¥–µ–ª–∞—Ç—å —Å–Ω–∏–º–æ–∫:**
            - –ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É –∫–∞–º–µ—Ä—ã –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–π –Ω–∞ —Å–¥–µ–ª–∞–Ω–Ω–æ–º —Ñ–æ—Ç–æ

            **üé• –ñ–∏–≤–∞—è –∫–∞–º–µ—Ä–∞:**
            - –ù–∞–∂–º–∏—Ç–µ "–ó–∞–ø—É—Å—Ç–∏—Ç—å –∂–∏–≤—É—é –∫–∞–º–µ—Ä—É" –¥–ª—è –Ω–∞—á–∞–ª–∞ —Å—Ç—Ä–∏–º–∞
            - –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —ç–º–æ—Ü–∏–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
            - –ù–∞–∂–º–∏—Ç–µ "–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∫–∞–º–µ—Ä—É" –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è

            **–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏:**
            - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–æ–ª–∑—É–Ω–∫–∏ –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–µ—Ç–µ–∫—Ü–∏–∏ –ª–∏—Ü
            - –î–ª—è –∂–∏–≤–æ–π –∫–∞–º–µ—Ä—ã: –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ FPS –∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏

            **–°–æ–≤–µ—Ç—ã –¥–ª—è –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:**
            - üì∏ –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –ª–∏—Ü–æ —Ö–æ—Ä–æ—à–æ –æ—Å–≤–µ—â–µ–Ω–æ
            - üë§ –°–º–æ—Ç—Ä–∏—Ç–µ –ø—Ä—è–º–æ –≤ –∫–∞–º–µ—Ä—É
            - üéØ –ò–∑–±–µ–≥–∞–π—Ç–µ —á–∞—Å—Ç–∏—á–Ω–æ–≥–æ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –ª–∏—Ü–∞
            - üìè –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –∫–∞–º–µ—Ä—ã: 0.5-2 –º–µ—Ç—Ä–∞
            - üòä –í—ã—Ä–∞–∂–∞–π—Ç–µ —ç–º–æ—Ü–∏–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ

            **–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —ç–º–æ—Ü–∏–∏:**
            - Angry (–≥–Ω–µ–≤)
            - Disgust (–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ)
            - Fear (—Å—Ç—Ä–∞—Ö)
            - Happy (—Å—á–∞—Å—Ç—å–µ)
            - Neutral (–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π)
            - Sad (–≥—Ä—É—Å—Ç—å)
            - Surprise (—É–¥–∏–≤–ª–µ–Ω–∏–µ)
            """)

def process_single_image(image_file, face_cascade, scale_factor, min_neighbors, model, device, img_size):
    try:

        image = Image.open(image_file).convert('RGB')
        image_array = np.array(image)
        image_bgr = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)

        gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(
            gray,
            scaleFactor=scale_factor,
            minNeighbors=min_neighbors,
            minSize=(30, 30)
        )

        result_image = image_bgr.copy()

        if len(faces) > 0:
            st.success(f"üéØ –ù–∞–π–¥–µ–Ω–æ –ª–∏—Ü: {len(faces)}")

            results = []
            for i, (x, y, w, h) in enumerate(faces):

                face_roi = image_bgr[y:y+h, x:x+w]

                probs = predict_on_bgr_image(face_roi, model, device, img_size)
                emotion_idx = np.argmax(probs)
                emotion = CLASSES[emotion_idx]
                confidence = float(probs[emotion_idx])

                results.append({
                    'face_num': i + 1,
                    'emotion': emotion,
                    'confidence': confidence,
                    'position': (x, y, w, h),
                    'all_probs': {CLASSES[j]: float(probs[j]) for j in range(len(CLASSES))}
                })

                cv2.rectangle(result_image, (x, y), (x+w, y+h), (0, 255, 0), 2)
                label = f'{emotion}: {confidence:.2%}'
                cv2.putText(result_image, label, (x, y-10),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)

            st.image(cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB),
                    caption="–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ—Ç–µ–∫—Ü–∏–∏", use_container_width=True)

            st.subheader("üìä –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")

            for result in results:
                with st.expander(f"–õ–∏—Ü–æ #{result['face_num']} - {result['emotion']} ({result['confidence']:.2%})"):
                    col1, col2 = st.columns(2)

                    with col1:
                        st.metric("–≠–º–æ—Ü–∏—è", result['emotion'])
                        st.metric("–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å", f"{result['confidence']:.2%}")
                        st.write(f"–ü–æ–∑–∏—Ü–∏—è: x={result['position'][0]}, y={result['position'][1]}")
                        st.write(f"–†–∞–∑–º–µ—Ä: {result['position'][2]}x{result['position'][3]}")

                    with col2:
                        st.write("**–í—Å–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏:**")
                        for emotion, prob in result['all_probs'].items():
                            st.write(f"- {emotion}: {prob:.2%}")

            st.subheader("üìà –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
            emotion_counts = {}
            for result in results:
                emotion = result['emotion']
                emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1

            if emotion_counts:

                stats_data = [
                    {'–≠–º–æ—Ü–∏—è': emotion, '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ': count, '–ü—Ä–æ—Ü–µ–Ω—Ç': f"{count/len(results)*100:.1f}%"}
                    for emotion, count in emotion_counts.items()
                ]
                stats_df = pd.DataFrame(stats_data)
                st.dataframe(stats_df, use_container_width=True)

                fig, ax = plt.subplots(figsize=(8, 4))
                emotions = list(emotion_counts.keys())
                counts = list(emotion_counts.values())

                colors = plt.cm.Set3(np.linspace(0, 1, len(emotions)))
                bars = ax.bar(emotions, counts, color=colors)

                ax.set_xlabel('–≠–º–æ—Ü–∏–∏')
                ax.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Ü')
                ax.set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —ç–º–æ—Ü–∏–π –Ω–∞ —Ñ–æ—Ç–æ')
                ax.grid(True, alpha=0.3)

                for bar, count in zip(bars, counts):
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height + 0.05,
                           f'{count}', ha='center', va='bottom')

                plt.xticks(rotation=45)
                st.pyplot(fig)

        else:
            st.warning("üòî –õ–∏—Ü–∞ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã")
            st.info("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–¥–µ–ª–∞—Ç—å —Ñ–æ—Ç–æ —Å —Ö–æ—Ä–æ—à–∏–º –æ—Å–≤–µ—â–µ–Ω–∏–µ–º –∏ —á–µ—Ç–∫–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º –ª–∏—Ü–∞")

            st.write("–ê–Ω–∞–ª–∏–∑ –≤—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
            probs = predict_on_bgr_image(image_bgr, model, device, img_size)
            emotion_idx = np.argmax(probs)
            emotion = CLASSES[emotion_idx]
            confidence = float(probs[emotion_idx])

            col1, col2 = st.columns(2)
            with col1:
                st.metric("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è —ç–º–æ—Ü–∏—è", emotion)
                st.metric("–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å", f"{confidence:.2%}")

            with col2:
                st.write("**–í—Å–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏:**")
                for i, prob in enumerate(probs):
                    st.write(f"- {CLASSES[i]}: {prob:.2%}")

            st.image(image, caption="–ò—Å—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", use_container_width=True)

    except Exception as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
        import traceback
        st.error(f"–î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏: {traceback.format_exc()}")

def create_complete_zip_report(all_results, df, winner, all_test_infos, test_path, img_size, batch_size, device):

    import zipfile
    import io

    print(f"DEBUG: –ù–∞—á–∏–Ω–∞–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ ZIP —Å {len(all_results)} –º–æ–¥–µ–ª–µ–π")
    print(f"DEBUG: DF shape: {df.shape}")

    try:

        zip_buffer = io.BytesIO()

        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
            print("DEBUG: ZIP —Ñ–∞–π–ª —Å–æ–∑–¥–∞–Ω, –Ω–∞—á–∏–Ω–∞–µ–º –¥–æ–±–∞–≤–ª—è—Ç—å –¥–∞–Ω–Ω—ã–µ")

            print("DEBUG: –°–æ–∑–¥–∞–µ–º CSV...")
            detailed_csv_data = []
            headers = [
                'Rank', 'Model', 'Accuracy', 'F1_Macro', 'Avg_ROC_AUC',
                'ROC_AUC_angry', 'ROC_AUC_disgust', 'ROC_AUC_fear', 'ROC_AUC_happy',
                'ROC_AUC_neutral', 'ROC_AUC_sad', 'ROC_AUC_surprise',
                'Precision_angry', 'Recall_angry', 'F1_angry',
                'Precision_disgust', 'Recall_disgust', 'F1_disgust',
                'Precision_fear', 'Recall_fear', 'F1_fear',
                'Precision_happy', 'Recall_happy', 'F1_happy',
                'Precision_neutral', 'Recall_neutral', 'F1_neutral',
                'Precision_sad', 'Recall_sad', 'F1_sad',
                'Precision_surprise', 'Recall_surprise', 'F1_surprise',
                'True_angry', 'True_disgust', 'True_fear', 'True_happy',
                'True_neutral', 'True_sad', 'True_surprise',
                'Pred_angry', 'Pred_disgust', 'Pred_fear', 'Pred_happy',
                'Pred_neutral', 'Pred_sad', 'Pred_surprise',
                'Support_angry', 'Support_disgust', 'Support_fear', 'Support_happy',
                'Support_neutral', 'Support_sad', 'Support_surprise',
                'Error_Status'
            ]
            detailed_csv_data.append(','.join(headers))

            print(f"DEBUG: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(df)} –º–æ–¥–µ–ª–µ–π –¥–ª—è CSV")
            for idx, row in df.iterrows():
                if idx % 5 == 0:
                    print(f"DEBUG: –û–±—Ä–∞–±–æ—Ç–∫–∞ –º–æ–¥–µ–ª–∏ {idx+1}/{len(df)}: {row['model']}")

                model_result = all_results[idx]
                model_name = row['model']

                base_data = [
                    str(row['rank']),
                    f'"{model_name}"',
                    f"{row['accuracy']:.6f}",
                    f"{row['f1_macro']:.6f}",
                    f"{row['roc_auc_avg']:.6f}"
                ]

                roc_auc_data = []
                if 'roc_auc' in model_result and isinstance(model_result['roc_auc'], dict):
                    for cls in CLASSES:
                        roc_auc_data.append(f"{model_result['roc_auc'].get(cls, 0):.6f}")
                else:
                    roc_auc_data = ['0.000000'] * len(CLASSES)

                precision_data = []
                recall_data = []
                f1_data = []
                support_data = []

                if 'report' in model_result and model_result['report']:
                    report_lines = model_result['report'].split('\n')
                    for line in report_lines:
                        if any(cls in line for cls in CLASSES):
                            parts = line.split()
                            if len(parts) >= 4:
                                try:
                                    cls_name = None
                                    for cls in CLASSES:
                                        if line.strip().startswith(cls):
                                            cls_name = cls
                                            break

                                    if cls_name:
                                        precision = float(parts[-4])
                                        recall = float(parts[-3])
                                        f1 = float(parts[-2])
                                        support = int(parts[-1])

                                        precision_data.append(f"{precision:.6f}")
                                        recall_data.append(f"{recall:.6f}")
                                        f1_data.append(f"{f1:.6f}")
                                        support_data.append(str(support))
                                except (ValueError, IndexError):
                                    precision_data.append("0.000000")
                                    recall_data.append("0.000000")
                                    f1_data.append("0.000000")
                                    support_data.append("0")

                while len(precision_data) < len(CLASSES):
                    precision_data.append("0.000000")
                    recall_data.append("0.000000")
                    f1_data.append("0.000000")
                    support_data.append("0")

                confusion_data = []
                if 'confusion' in model_result and model_result['confusion'] is not None:
                    cm = model_result['confusion']
                    if isinstance(cm, np.ndarray):
                        for i in range(len(CLASSES)):
                            confusion_data.append(str(int(cm[i].sum())))
                        for j in range(len(CLASSES)):
                            confusion_data.append(str(int(cm[:, j].sum())))
                else:
                    confusion_data = ['0'] * (len(CLASSES) * 2)

                error_status = model_result.get('error', 'Success')
                if error_status:
                    error_status = f'"{error_status}"'
                else:
                    error_status = 'Success'

                full_row = base_data + roc_auc_data + precision_data + recall_data + f1_data + confusion_data + support_data + [error_status]
                detailed_csv_data.append(','.join(full_row))

            detailed_csv_text = '\n'.join(detailed_csv_data)
            zip_file.writestr("models_super_detailed_report.csv", detailed_csv_text)
            print("DEBUG: CSV –¥–æ–±–∞–≤–ª–µ–Ω –≤ ZIP")

            print("DEBUG: –î–æ–±–∞–≤–ª—è–µ–º ROC –∫—Ä–∏–≤—ã–µ...")
            for idx, row in df.iterrows():
                model_result = all_results[idx]
                model_name = row['model'].replace('.pth', '')

                if 'fig_roc' in model_result and model_result['fig_roc']:
                    img_buffer = io.BytesIO()
                    model_result['fig_roc'].savefig(img_buffer, format='png', dpi=150, bbox_inches='tight')
                    img_buffer.seek(0)

                    zip_file.writestr(f"roc_curves/roc_curve_{model_name}.png", img_buffer.getvalue())

                    metadata = f
                    if 'roc_auc' in model_result and isinstance(model_result['roc_auc'], dict):
                        for cls, auc_val in model_result['roc_auc'].items():
                            metadata += f"{cls}: {auc_val:.6f}\n"

                    zip_file.writestr(f"roc_curves/roc_metadata_{model_name}.txt", metadata)

            readme_content = f
            zip_file.writestr("README.txt", readme_content)
            print("DEBUG: README –¥–æ–±–∞–≤–ª–µ–Ω –≤ ZIP")

        zip_buffer.seek(0)
        result = zip_buffer.getvalue()
        print(f"DEBUG: ZIP —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω, —Ä–∞–∑–º–µ—Ä: {len(result)} –±–∞–π—Ç")
        return result

    except Exception as e:
        print(f"DEBUG: –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ ZIP: {e}")
        import traceback
        print(f"DEBUG: Traceback: {traceback.format_exc()}")
        raise e

if __name__ == '__main__':
    main()
